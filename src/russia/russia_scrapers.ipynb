{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CNN",
   "id": "d01e9e716461189a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-06T23:35:57.420085Z",
     "start_time": "2025-01-06T23:32:06.888951Z"
    }
   },
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "#! THIS CODE ONLY GETS URLS\n",
    "\n",
    "# Set up the Selenium WebDriver (you need to have ChromeDriver installed for Chrome)\n",
    "driver = webdriver.Chrome()\n",
    "keywords = [\n",
    "    # Geopolitical Context\n",
    "    \"Russia\",\n",
    "    \"Ukraine\",\n",
    "    \"Moscow\",\n",
    "    \"Kyiv\",\n",
    "    \"Voronezh\",\n",
    "    \"Crimea\",\n",
    "    \"Kharkiv\",\n",
    "    \"Donetsk\",\n",
    "    \"Luhansk\",\n",
    "    \"Putin\",\n",
    "    \"Zelensky\"]\n",
    "\n",
    "url_set = []\n",
    "# Load the CNN search page\n",
    "for query in keywords:\n",
    "    print(f'Starting to search for the keyword: \"{query}\"')\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        url = f'https://www.cnn.com/search?q={query}&from={i * 90}&size=90&sort=newest&types=all&section='\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"container__field-links\"))\n",
    "            )\n",
    "        except:\n",
    "            print(\"Timeout or no content available. Exiting loop.\")\n",
    "            break\n",
    "\n",
    "        # Parse with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        link_container = soup.find(\"div\", class_=\"container__field-links container_list-images-with-description__field-links\")\n",
    "\n",
    "        if link_container:\n",
    "            results = link_container.find_all('a', href=True)\n",
    "            links = [link.get('href') for link in results if 'index.html' in link.get('href')]\n",
    "            url_set.extend(links)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "driver.quit()\n",
    "print(f\"Got {len(url_set)} urls BEFORE removing duplicates\")\n",
    "links_list = list(set(url_set))\n",
    "print(f\"Got {len(links_list)} urls AFTER removing duplicates\")\n",
    "# with open('urls.txt', 'w') as f:\n",
    "#     for url in links_list:\n",
    "#         f.write(f\"{url}\\n\")\n",
    "data = {\n",
    "    \"cnn.com\": links_list\n",
    "}\n",
    "with open('data/article_urls.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to search for the keyword: \"Russia\"\n",
      "Timeout or no content available. Exiting loop.\n",
      "Starting to search for the keyword: \"Ukraine\"\n",
      "Timeout or no content available. Exiting loop.\n",
      "Starting to search for the keyword: \"Moscow\"\n",
      "Timeout or no content available. Exiting loop.\n",
      "Starting to search for the keyword: \"Kyiv\"\n",
      "Timeout or no content available. Exiting loop.\n",
      "Starting to search for the keyword: \"Voronezh\"\n",
      "Timeout or no content available. Exiting loop.\n",
      "Starting to search for the keyword: \"Crimea\"\n",
      "Timeout or no content available. Exiting loop.\n",
      "Starting to search for the keyword: \"Kharkiv\"\n",
      "Timeout or no content available. Exiting loop.\n",
      "Starting to search for the keyword: \"Donetsk\"\n",
      "Timeout or no content available. Exiting loop.\n",
      "Starting to search for the keyword: \"Luhansk\"\n",
      "Timeout or no content available. Exiting loop.\n",
      "Starting to search for the keyword: \"Putin\"\n",
      "Timeout or no content available. Exiting loop.\n",
      "Starting to search for the keyword: \"Zelensky\"\n",
      "Timeout or no content available. Exiting loop.\n",
      "Got 5118 urls BEFORE removing duplicates\n",
      "Got 1428 urls AFTER removing duplicates\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# NYPost",
   "id": "a1632155ea978646"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T23:44:39.879188Z",
     "start_time": "2025-01-06T23:35:57.420085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "#! THIS CODE ONLY GETS URLS\n",
    "\n",
    "# Set up the Selenium WebDriver (you need to have ChromeDriver installed for Chrome)\n",
    "driver = webdriver.Chrome()\n",
    "query_list = [\n",
    "    # Geopolitical Context\n",
    "    \"Russia\",\n",
    "    \"Ukraine\",\n",
    "    \"Moscow\",\n",
    "    \"Kyiv\",\n",
    "    \"Voronezh\",\n",
    "    \"Crimea\",\n",
    "    \"Kharkiv\",\n",
    "    \"Donetsk\",\n",
    "    \"Luhansk\",\n",
    "    \"Putin\",\n",
    "    \"Zelensky\"]\n",
    "\n",
    "url_set = []\n",
    "\n",
    "# Load the CNN search page\n",
    "for query in query_list:\n",
    "    print(f'Starting to query \"{query}\"')\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        url = f'https://nypost.com/search/{query}/page/{i}/'\n",
    "\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            WebDriverWait(driver, 30).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"page__content\"))\n",
    "            )\n",
    "        except:\n",
    "            print(\"Timeout or no content available. Exiting loop.\")\n",
    "            break\n",
    "\n",
    "        # Parse with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        story_links = [a['href'] for a in soup.select('.search-results__story .story__headline a')]\n",
    "\n",
    "        if story_links:\n",
    "            url_set.extend(story_links)\n",
    "            print(f\"Got {len(story_links)}!\")\n",
    "        else:\n",
    "            break\n",
    "\n",
    "driver.quit()\n",
    "print(f\"got {len(url_set)} links!\")\n",
    "\n",
    "links_list = list(set(url_set))\n",
    "with open('urls.txt', 'a') as f:\n",
    "    for url in links_list:\n",
    "        f.write(f\"{url}\\n\")\n",
    "\n",
    "data = {\n",
    "    \"nypost.com\": links_list\n",
    "}\n",
    "with open('data/article_urls.json', 'w') as f:\n",
    "    json.dumps(data, f, indent=4)\n",
    "\n"
   ],
   "id": "a6767f0349ec17f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to query \"Russia\"\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Timeout or no content available. Exiting loop.\n",
      "Starting to query \"Ukraine\"\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Got 20!\n",
      "Timeout or no content available. Exiting loop.\n",
      "Starting to query \"Moscow\"\n",
      "Got 20!\n",
      "Got 20!\n",
      "Timeout or no content available. Exiting loop.\n",
      "Starting to query \"Kyiv\"\n",
      "Timeout or no content available. Exiting loop.\n",
      "Starting to query \"Voronezh\"\n",
      "Timeout or no content available. Exiting loop.\n",
      "Starting to query \"Crimea\"\n",
      "Timeout or no content available. Exiting loop.\n",
      "Starting to query \"Kharkiv\"\n",
      "Timeout or no content available. Exiting loop.\n",
      "Starting to query \"Donetsk\"\n",
      "Timeout or no content available. Exiting loop.\n",
      "Starting to query \"Luhansk\"\n",
      "Timeout or no content available. Exiting loop.\n",
      "Starting to query \"Putin\"\n",
      "Timeout or no content available. Exiting loop.\n",
      "Starting to query \"Zelensky\"\n",
      "Timeout or no content available. Exiting loop.\n",
      "got 2960 links!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dumps() takes 1 positional argument but 2 positional arguments (and 1 keyword-only argument) were given",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 68\u001B[0m\n\u001B[0;32m     64\u001B[0m data \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnypost.com\u001B[39m\u001B[38;5;124m\"\u001B[39m: links_list\n\u001B[0;32m     66\u001B[0m }\n\u001B[0;32m     67\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/article_urls.json\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m---> 68\u001B[0m     \u001B[43mjson\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdumps\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: dumps() takes 1 positional argument but 2 positional arguments (and 1 keyword-only argument) were given"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Spectator",
   "id": "a01ee09e3a14d695"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T23:44:39.880701Z",
     "start_time": "2025-01-06T23:44:39.879188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "keywords = [\n",
    "    # Geopolitical Context\n",
    "    \"Russia\",\n",
    "    \"Ukraine\",\n",
    "    \"Moscow\",\n",
    "    \"Kyiv\",\n",
    "    \"Voronezh\",\n",
    "    \"Crimea\",\n",
    "    \"Kharkiv\",\n",
    "    \"Donetsk\",\n",
    "    \"Luhansk\",\n",
    "    \"Putin\",\n",
    "    \"Zelensky\"]\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "url_set = []\n",
    "cut_off_date = datetime(2023,10, 7)\n",
    "article_counter = 0\n",
    "\n",
    "for query in keywords:\n",
    "    print(f\"Starting to search for {query}\")\n",
    "    condition = True\n",
    "    i = 1\n",
    "    while condition:\n",
    "\n",
    "        url = f'https://www.spectator.co.uk/?s={query}&page={i}'\n",
    "        driver.get(url)\n",
    "\n",
    "        try:\n",
    "            # Wait for the elements to be visible\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article\"))\n",
    "            )\n",
    "        except:\n",
    "            print(\"Timeout or no content available. Exiting loop.\")\n",
    "            condition = False\n",
    "            break\n",
    "        #Parsing with bs4\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "        #Identifying newspapers to scrape\n",
    "        news_blocks = soup.find_all('article')\n",
    "        for block in news_blocks:\n",
    "\n",
    "            #retrieving the date of posting\n",
    "            #Some recent papers instead mark \"X hours ago\"\n",
    "\n",
    "            date_element = block.find('header', attrs = {\"class\":'search-card__header'}).span\n",
    "            if date_element is not None:\n",
    "                date_element = date_element.text\n",
    "                try:\n",
    "                    post_date = datetime.strptime(date_element, \"%d %B %Y\")\n",
    "\n",
    "                except ValueError: #this is for newspapers that have \"X hours ago\"\n",
    "                    post_date = datetime.now()\n",
    "\n",
    "                if post_date < cut_off_date:  #stop scraping once no longer in time frame of interest\n",
    "                    condition = False\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    link_tag = block.find('a', {'class': 'search-card__title-link'})\n",
    "                    link = link_tag.get('href')\n",
    "                    if 'article' in link: #avoids podcasts\n",
    "                        url_set.append(link)\n",
    "                        article_counter += 1\n",
    "            else:\n",
    "                pass #skip newspapers that do not have a posted date\n",
    "\n",
    "        i+=1\n",
    "\n",
    "driver.quit()\n",
    "print('Scraped a total of: ', article_counter, 'articles!')\n",
    "\n",
    "no_duplicate_url_set = list(set(url_set))\n",
    "\n",
    "data = {\n",
    "    \"https://www.chicagotribune.com/\": no_duplicate_url_set\n",
    "}\n",
    "with open(r'data/article_urls.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ],
   "id": "ad96823ba567fc7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Washington Post",
   "id": "a447297166eca6df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T23:44:39.880701Z",
     "start_time": "2025-01-06T23:44:39.880701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2024-11-12\n",
    "# Jacob Sauvé\n",
    "# Washington Post Scraper\n",
    "\n",
    "# ! THIS CODE ONLY GETS URLS !\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import time as t\n",
    "\n",
    "# Constants\n",
    "NEWS_SITE = \"www.washingtonpost.com\"\n",
    "NEXT_BUTTON_TXT = \"//*[contains(text(), 'Load more results')]\"\n",
    "COOKIES_POP_OVER_TXT = \"//*[contains(text(), 'Reject All')]\"\n",
    "ARTICLES_PER_KEYWORD = 10000 # Must be an integer, ideally 10n\n",
    "CLICK_DELAY = 0.5 # In seconds, time between auto clicks to allow page to load\n",
    "\n",
    "keywords = [\n",
    "    # Geopolitical Context\n",
    "    \"Russia\",\n",
    "    \"Ukraine\",\n",
    "    \"Moscow\",\n",
    "    \"Kyiv\",\n",
    "    \"Voronezh\",\n",
    "    \"Crimea\",\n",
    "    \"Kharkiv\",\n",
    "    \"Donetsk\",\n",
    "    \"Luhansk\",\n",
    "    \"Putin\",\n",
    "    \"Zelensky\"]\n",
    "\n",
    "url_set = list() # Set of scraped URLs\n",
    "\n",
    "\n",
    "for keyword in keywords:\n",
    "    # Set up WebDriver for Firefox (with custom user-agent to evade protection)\n",
    "    options = Options()\n",
    "    options.set_preference(\n",
    "        \"general.useragent.override\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, \\\n",
    "    like Gecko) Firefox/91.0 Safari/537.36\"\n",
    "    ) # Custom user agent to appear less like a bot\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    url = f\"https://www.washingtonpost.com/search/?query={keyword}\"\n",
    "    driver.get(url)\n",
    "\n",
    "    print(f\"\\nBeginning scraping of {NEWS_SITE} for keyword '{keyword}'!\")\n",
    "\n",
    "    # Check for cookie acceptance pop-up and click \"Reject All\"\n",
    "    try:\n",
    "        cookies_button = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, COOKIES_POP_OVER_TXT))\n",
    "        )\n",
    "        cookies_button.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # While the 'Load More' button exists, click on it, THEN scrape all URLs\n",
    "    # Actually - only scraping a maximum of ARTICLES_PER_KEYWORD articles\n",
    "    for load_more in range((ARTICLES_PER_KEYWORD)//10-1):\n",
    "        t.sleep(CLICK_DELAY)\n",
    "        # Check if more results exist\n",
    "        try:\n",
    "            next_button = WebDriverWait(driver, 10).until(\n",
    "                # Check for Next button\n",
    "                EC.presence_of_element_located((By.XPATH, NEXT_BUTTON_TXT))\n",
    "            )\n",
    "            next_button.click()\n",
    "            print(f\"{(1+load_more)*10} articles loaded...\")\n",
    "        except:\n",
    "            # Occurs if the button did not load, i.e. no more search results \n",
    "            print(\"Error: Timeout or no button found. Exiting loop...\")\n",
    "            break\n",
    "\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser') # Beautify the HTML\n",
    "    # Select 2nd section in HTML page (the link container)\n",
    "    link_container = soup.find_all(\"section\")[1]\n",
    "\n",
    "    if link_container:\n",
    "        # Scrape all articles\n",
    "        results = link_container.find_all('a', href=True)\n",
    "        links = [\n",
    "            link.get('href') for link in results if link.get('href')\n",
    "        ]\n",
    "        print(f\"Total: {len(links)} URLs scraped\")\n",
    "        url_set.extend(links)\n",
    "        print(\"URLs successfully scraped!\")\n",
    "    else:\n",
    "        print(\"Error: Link container was not found.\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "url_list = list(set(url_set)) # Remove link duplicates\n",
    "print(f\"Number of articles: {len(url_list)}\")\n",
    "# Name file with timestamp for differentiation\n",
    "path = t.strftime(\n",
    "    f'news-bias-model/src/data/{NEWS_SITE}_%y-%m-%d-%Hh%Mmin%Ss_urls.txt',\n",
    "    t.localtime()\n",
    ")\n",
    "# Save URLs to a .txt file, to be converted to json later\n",
    "with open(path, 'a') as file:\n",
    "    urls_chained = '\"' + '\", \"'.join(url_list) + '\"'\n",
    "    file.write(f'\"{NEWS_SITE}\":[{urls_chained}]')\n",
    "\n"
   ],
   "id": "96f4f8c5f97ec548",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# AP News",
   "id": "23c33c00f75979fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import json\n",
    "\n",
    "urls = [\"https://apnews.com/\",\n",
    "        \"https://apnews.com/world-news\",\n",
    "        \"https://apnews.com/us-news\",\n",
    "        \"https://apnews.com/politics\",\n",
    "        \"https://apnews.com/sports\",\n",
    "        \"https://apnews.com/entertainment\",\n",
    "        \"https://apnews.com/business\",\n",
    "        \"https://apnews.com/science\",\n",
    "        \"https://apnews.com/ap-fact-check\",\n",
    "        \"https://apnews.com/oddities\",\n",
    "        \"https://apnews.com/hub/be-well\",\n",
    "        \"https://apnews.com/newsletters\",\n",
    "        \"https://apnews.com/photography\",\n",
    "        \"https://apnews.com/hub/ap-investigations/\",\n",
    "        \"https://apnews.com/health\",\n",
    "        \"https://apnews.com/technology\",\n",
    "        \"https://apnews.com/lifestyle\",\n",
    "        \"https://apnews.com/religion\",\n",
    "        \"https://apnews.com/climate-and-environment\",\n",
    "        \"https://apnews.com/hub/mideast-wars\",\n",
    "        \"https://apnews.com/hub/global-elections/\",\n",
    "        \"https://apnews.com/hub/noticias\",\n",
    "        \"https://apnews.com/hub/china\",\n",
    "        \"https://apnews.com/hub/asia-pacific\",\n",
    "        \"https://apnews.com/hub/latin-america\",\n",
    "        \"https://apnews.com/hub/europe\",\n",
    "        \"https://apnews.com/hub/africa\",\n",
    "        \"https://apnews.com/hub/australia\",\n",
    "        \"https://apnews.com/hub/middle-east\",\n",
    "        \"https://apnews.com/hub/russia-ukraine\",\n",
    "        \"https://apnews.com/hub/israel-hamas-war\"\n",
    "        ]\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "num = 0\n",
    "all_articles = []\n",
    "\n",
    "try:\n",
    "    for url in urls:\n",
    "        driver.get(url)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                # Wait until the \"Load More\" button is present\n",
    "                load_more_button = wait.until(\n",
    "                    EC.presence_of_element_located((By.XPATH, \"//a[@class='Button' and contains(text(), 'Load More')]\"))\n",
    "                )\n",
    "\n",
    "                # Scroll the \"Load More\" button into view\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView(true);\", load_more_button)\n",
    "                time.sleep(1)  # Allow time for scroll to finish\n",
    "\n",
    "                # Try to click the \"Load More\" button using JavaScript\n",
    "                driver.execute_script(\"arguments[0].click();\", load_more_button)\n",
    "                time.sleep(2)  # Wait for content to load after clicking\n",
    "\n",
    "            except Exception as e:\n",
    "                break\n",
    "\n",
    "        # Get the page source after clicking and loading new content\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Extract article links\n",
    "        article_links = set()\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if '/article' in href:\n",
    "                full_url = f\"https://apnews.com{href}\" if href.startswith('/') else href\n",
    "                article_links.add(full_url)\n",
    "\n",
    "        for article in article_links:\n",
    "            all_articles.append(article)\n",
    "            num += 1\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "with open('ap_news.json', 'w') as json_file:\n",
    "    json.dump(all_articles, json_file, indent=4)\n",
    "\n",
    "print(f\"Total articles found: {num}\")\n"
   ],
   "id": "b77cd4decbc45dba"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
