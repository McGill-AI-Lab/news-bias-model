{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eaba59c3bf0d827",
   "metadata": {},
   "source": [
    "What we need to do: \n",
    "1) Go through each newspaper and create a newspaper corpora, display the length of each corpora\n",
    "2) Find either a) wikipedia dataset and train word2vec on it to compare non-context related words like \"bad\", \n",
    "\"victim\", \"good\", \"attacker\" to representation of each side of the conflict in the newspaper corpora, b) find a \n",
    "pretrained word2vec to do the same\n",
    "3) Find a way to classify each article as pro-israel or pro-palestine"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Access and preprocess our data",
   "id": "8de74c7d7aa56c9d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open('data/news-data-extracted.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Print the data\n",
    "first_article_data = data[\"cnn.com\"][0] #cnn is the key to a value which is a list of dictionaries, we get the first dictionary (article) of that list of dictionary\n",
    "first_article = first_article_data[\"text\"]\n",
    "print(first_article_data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d65473b172ae9cd",
   "metadata": {},
   "source": [
    "first_article[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "309ac0a633e6010e",
   "metadata": {},
   "source": [
    "from gensim.utils import tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# tokenizes article into sentences, which are also tokenized into words\n",
    "def tokenize_article(article):\n",
    "    tokenized_article = []\n",
    "    sentences  = sent_tokenize(article, language=\"english\") # divide article into sentences\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokenized_sentence = tokenize(sentence) # divide sentences into words\n",
    "        tokenized_article.append(tokenized_sentence) \n",
    "    return tokenized_article\n",
    "\n",
    "# makes each word lowercase\n",
    "def lowercase(tokenized_article):\n",
    "    lowercase_article = []\n",
    "\n",
    "    for sentence in tokenized_article:\n",
    "        current_sentence = []\n",
    "        for word in sentence:\n",
    "            current_sentence.append(word.lower())\n",
    "        lowercase_article.append(current_sentence)\n",
    "\n",
    "    return lowercase_article\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(tokenized_article):\n",
    "    # Iterate over the index and content of each sentence\n",
    "    for i in range(len(tokenized_article)):\n",
    "        # Create a new list for the filtered sentence\n",
    "        filtered_sentence = []\n",
    "        for word in tokenized_article[i]:\n",
    "            if word not in stop_words:\n",
    "                filtered_sentence.append(word)\n",
    "        # Replace the original sentence with the filtered sentence\n",
    "        tokenized_article[i] = filtered_sentence\n",
    "    return tokenized_article\n",
    "\n",
    "def lammetization(tokenized_article):\n",
    "    lammetizer = WordNetLemmatizer()\n",
    "\n",
    "    lammetized_article = []\n",
    "\n",
    "    for sentence in tokenized_article:\n",
    "        current_sentence = []\n",
    "        for word in sentence:\n",
    "            current_sentence.append(lammetizer.lemmatize(word))\n",
    "        lammetized_article.append(current_sentence)\n",
    "\n",
    "    return lammetized_article\n",
    "\n",
    "\n",
    "def remove_punctuation(tokenized_article):\n",
    "    punc_removed_article = []\n",
    "\n",
    "    for sentence in tokenized_article:\n",
    "        punc_removed_sentence = []\n",
    "        for word in sentence:\n",
    "            # Split by punctuation, filter out empty strings, and join back if needed\n",
    "            split_word = ''.join(re.split(r\"[^\\w]+\", word))\n",
    "            if split_word:  # Add non-empty words only\n",
    "                punc_removed_sentence.append(split_word)\n",
    "\n",
    "        punc_removed_article.append(punc_removed_sentence)\n",
    "\n",
    "    return punc_removed_article\n",
    "\n",
    "def preprocess_article(article):\n",
    "    t_article = tokenize_article(article)\n",
    "    l_article = lowercase(t_article)\n",
    "    r_article = remove_stopwords(l_article)\n",
    "    la_article = lammetization(r_article)\n",
    "    re_article = remove_punctuation(la_article)\n",
    "    return re_article"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(stop_words)\n",
    "print(stopwords.words(\"english\"))\n",
    "print(set(stopwords.words(\"english\")))"
   ],
   "id": "6278a1090040ea31",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e6a5c2a3d799cc88",
   "metadata": {},
   "source": [
    "preprocessed = preprocess_article(first_article)\n",
    "print(preprocessed)\n",
    "for word in stopwords.words(\"english\"):\n",
    "    if word in preprocessed:\n",
    "        print(word)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "80ae5791093d845d",
   "metadata": {},
   "source": [
    "Now create a function that preperocesses a newspaper"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# def create_article_list(extracted_file, newspaper_name):\n",
    "#     \"\"\" Takes in the file of extracted news and the newspaper name\n",
    "#     Outputs an artilce (text) list for the given newspaper\n",
    "#     \"\"\"\n",
    "#     import json\n",
    "#     with open(extracted_file, \"r\") as json_file:\n",
    "#         data = json.load(json_file)\n",
    "#\n",
    "#     newspaper = data[newspaper_name] # newspaper will be a dictionary of articles with values being url, date, authors, text etc.\n",
    "#     newspaper_articles = []\n",
    "#\n",
    "#     for article in newspaper:\n",
    "#         if article != None:\n",
    "#             newspaper_articles.append(f\"{article[\"text\"]}\")\n",
    "#\n",
    "#     print(newspaper_articles)\n",
    "\n",
    "def create_article_list(extracted_file, newspaper_name):\n",
    "    \"\"\"\n",
    "    Takes in the file of extracted news and the newspaper name.\n",
    "    Outputs an article (text) list for the given newspaper.\n",
    "    \"\"\"\n",
    "    import json\n",
    "\n",
    "    # Load the JSON file\n",
    "    with open(extracted_file, \"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    # Extract newspaper data\n",
    "    newspaper = data.get(newspaper_name, [])  # Default to an empty list if not found\n",
    "    newspaper_articles = []\n",
    "\n",
    "    # Loop through articles in the newspaper\n",
    "    for article in newspaper:\n",
    "        # Check if article has a valid \"text\" key\n",
    "        if article and isinstance(article, dict) and \"text\" in article:\n",
    "            newspaper_articles.append(article[\"text\"])  # Use append to add the text to the list\n",
    "\n",
    "    print(f\"Extracted {len(newspaper_articles)} articles from {newspaper_name}.\")\n",
    "    return newspaper_articles\n",
    "\n"
   ],
   "id": "953133fe35df9b92",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "94847e14cc9be247",
   "metadata": {},
   "source": [
    "def preprocess_newspaper(article_list):\n",
    "    \"\"\" Takes in article list and give back a list of list which is preprocessed article in the form of every element in the list is a sentence which consist of lists of words\"\"\"\n",
    "\n",
    "    if not article_list:  # Handle empty or None input\n",
    "        print(\"No articles provided for preprocessing.\")\n",
    "        return []\n",
    "\n",
    "    preprocessed_article_list = []\n",
    "    i = 0 # to see how many articles we processed\n",
    "\n",
    "    for article in article_list:\n",
    "        preprocessed_article_list.extend(preprocess_article(article))  # extends preproccessed\n",
    "        # articles to newspaper's article list\n",
    "        print(f\"article {i} preprocessed\")\n",
    "        i += 1\n",
    "\n",
    "    return preprocessed_article_list"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "53184256486a4c43",
   "metadata": {},
   "source": [
    "# Lets try with CNN\n",
    "article_list = create_article_list(\"data/news-data-extracted.json\", \"cnn.com\")\n",
    "print(article_list)\n",
    "preprocessed = preprocess_newspaper(article_list)\n",
    "print(preprocessed)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# create get_article_number, corpussize, and other helper functions\n",
    "\n",
    "def no_of_articles(article_list):\n",
    "    return len(article_list)\n",
    "\n",
    "def corpus_size_before(article_list):\n",
    "    corpus = article_list\n",
    "\n",
    "    corpus_size = 0\n",
    "    for article in article_list:\n",
    "        corpus_size += len(article.split())\n",
    "\n",
    "    return corpus_size\n",
    "\n",
    "\n",
    "def corpus_size_after(preprocessed_article_list):\n",
    "    corpus = preprocessed_article_list\n",
    "\n",
    "    corpus_size = 0\n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            corpus_size += 1\n",
    "\n",
    "    return corpus_size\n",
    "\n",
    "\n",
    "def no_of_unique_words(preprocessed_article_list):\n",
    "    words = []\n",
    "\n",
    "    for sentence in preprocessed_article_list:\n",
    "        for word in sentence:\n",
    "            if word in words:\n",
    "                pass\n",
    "            else:\n",
    "                words.append(word)\n",
    "\n",
    "    return len(words)\n",
    "\n",
    "def no_of_sentences(preprocessed_article_list):\n",
    "    return len(preprocessed_article_list)"
   ],
   "id": "8430fb4a02dc172c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\" Occurance Counter \"\"\"\n",
    "\n",
    "# Palestine\n",
    "def occurance(target_word, preprocessed_article_list):\n",
    "    counter = 0\n",
    "\n",
    "    for sentence in preprocessed_article_list:\n",
    "        for word in sentence:\n",
    "            if word == f\"{target_word}\":\n",
    "                counter += 1\n",
    "    \n",
    "    return counter"
   ],
   "id": "c14e2862eda84853",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training function",
   "id": "8d5de139cac5f3af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train(newspaper_name, sentence_list):\n",
    "    from gensim.models import Word2Vec\n",
    "    import os\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(newspaper_name, exist_ok=True)\n",
    "\n",
    "    # Train Word2Vec model\n",
    "    # Initialize the model with parameters\n",
    "    model = Word2Vec(sentences=sentence_list, vector_size=300, window=5, min_count=10, sg=1, workers=4, negative=20)\n",
    "\n",
    "    # Train and save the model\n",
    "    model.train(sentence_list, total_examples=len(sentence_list), epochs=20)\n",
    "    model.save(os.path.join(newspaper_name, f\"{newspaper_name}_w2v.model\"))\n",
    "\n",
    "    # # Save just the word vectors in a text and binaryformat\n",
    "    # model.wv.save_word2vec_format(f\"{newspaper_name/newspaper_name}_w2v_vectors.txt\", binary=False)\n",
    "    # model.wv.save_word2vec_format(f\"{newspaper_name/newspaper_name}_w2v_vectors.bin\", binary=True)\n",
    "\n",
    "\n",
    "    import os\n",
    "\n",
    "    model.wv.save_word2vec_format(os.path.join(newspaper_name, f\"{newspaper_name}_w2v_vectors.txt\"),binary=False)\n",
    "    model.wv.save_word2vec_format(os.path.join(newspaper_name, f\"{newspaper_name}_w2v_vectors.bin\"),binary=True)\n",
    "\n",
    "\n",
    "    return (\n",
    "        os.path.join(newspaper_name, f\"{newspaper_name}_w2v.model\"),\n",
    "        os.path.join(newspaper_name, f\"{newspaper_name}_w2v_vectors.txt\"),\n",
    "        os.path.join(newspaper_name, f\"{newspaper_name}_w2v_vectors.bin\"),\n",
    "    )\n"
   ],
   "id": "6255942a449e9d4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Calculate portrayal",
   "id": "85f72616c7809892"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_portrayal(model, palestinian_words, israeli_words, positive_portrayal_words, negative_portrayal_words): # target_words and portrayal_words are lists\n",
    "    palestine_portrayal_scores = {}\n",
    "    israel_portrayal_scores = {}\n",
    "\n",
    "    # Access the list of words in the vocabulary\n",
    "    vocabulary_words = list(model.wv.key_to_index.keys())\n",
    "    \n",
    "    # no of portrayal words\n",
    "    pos_count = 0\n",
    "    for word in positive_portrayal_words:\n",
    "        if word in vocabulary_words:\n",
    "            pos_count += 1\n",
    "    neg_count = 0\n",
    "    for word in negative_portrayal_words:\n",
    "        if word in vocabulary_words:\n",
    "            neg_count += 1       \n",
    "\n",
    "\n",
    "    for word in palestinian_words:\n",
    "            palestine_portrayal_scores[word] = 0\n",
    "            for positive in positive_portrayal_words:\n",
    "                if positive in vocabulary_words:\n",
    "                    palestine_portrayal_scores[word] += (model.wv.similarity(f\"{word}\", f\"{positive}\")/pos_count)\n",
    "            for negative in negative_portrayal_words:\n",
    "                if positive in vocabulary_words:\n",
    "                    palestine_portrayal_scores[word] -= (model.wv.similarity(f\"{word}\", f\"{negative}\")/neg_count)\n",
    "\n",
    "    for word in israeli_words:\n",
    "        israel_portrayal_scores[word] = 0\n",
    "        for positive in positive_portrayal_words:\n",
    "            if positive in vocabulary_words:\n",
    "                israel_portrayal_scores[word] += (model.wv.similarity(f\"{word}\", f\"{positive}\")/pos_count)\n",
    "        for negative in negative_portrayal_words:\n",
    "            if positive in vocabulary_words:\n",
    "                israel_portrayal_scores[word] -= (model.wv.similarity(f\"{word}\", f\"{negative}\")/neg_count)\n",
    "\n",
    "    return palestine_portrayal_scores, israel_portrayal_scores"
   ],
   "id": "6cd127c8489a891d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Should I include gaza, if yes, add an occurance function and add it to the target word_list and portrayal",
   "id": "57b27bf3467fde6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Save newspaper dictionary",
   "id": "3a0781d7cf229880"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def save_newspaper_dict(newspaper_dict):\n",
    "    # File path for the JSON file\n",
    "    file_path = \"preprocessed_newspapers_dict.json\"\n",
    "\n",
    "    # Open the JSON file\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        data = json.load(json_file)  # Load existing data\n",
    "\n",
    "    # Iterate over items in the dictionary\n",
    "    for key, value in newspaper_dict.items():  # Use .items() to get key-value pairs\n",
    "        if key not in data:\n",
    "            data[key] = value  # Save new key-value pair\n",
    "\n",
    "    # Save updated data back to the file\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(data, json_file, indent=4)"
   ],
   "id": "82993f5040b960cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def load_preprocessed_newspapers(json_file):\n",
    "    \"\"\"\n",
    "    Load preprocessed newspapers from a JSON file.\n",
    "    \"\"\"\n",
    "    if os.path.exists(json_file):\n",
    "        try:\n",
    "            with open(json_file, 'r') as file:\n",
    "                data = json.load(file)\n",
    "                if isinstance(data, dict):\n",
    "                    print(f\"Successfully loaded preprocessed newspapers from {json_file}.\")\n",
    "                    return data\n",
    "                else:\n",
    "                    print(\"Error: JSON data is not a dictionary. Returning an empty dictionary.\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON file {json_file}: {e}\")\n",
    "    else:\n",
    "        print(f\"File {json_file} does not exist. Starting with an empty dictionary.\")\n",
    "\n",
    "    return {}\n"
   ],
   "id": "f1ee0f4d346df856",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "newspaper_list = [\"cnn.com\", \"WashingtonPost.com\"]\n",
    "\n",
    "def master(extracted_file, newspaper_list):\n",
    "    \"\"\"\n",
    "    Get a list of newspapers\n",
    "    Create a dictionary of newspapers, which is a dictionary\n",
    "    For every newspaper, have the following keys:\n",
    "    # of articles, corpus size (before preprocessing), # of unique words (before preprocessing),\n",
    "    list of articles (only text) (before preprocessing),\n",
    "    preprocessed articles (a list of sentences, which are a list of words)\n",
    "    corpus size (after preprocessing), # of unique words (after preprocessing),\n",
    "    how many times each target word appears (palestine, israel, hamas, idf, netanyahu, sinwar, etc.)\n",
    "    train a word2vec, save the model and the weights,\n",
    "    bias score for palestine, israel, hamas, idf, etc,\n",
    "    Add the following key to each articles\n",
    "    \"\"\"\n",
    "    from gensim.models import KeyedVectors\n",
    "    from gensim.models import Word2Vec\n",
    "\n",
    "    preprocessed_newspapers = load_preprocessed_newspapers(\"preprocessed_newspapers_dict.json\")\n",
    "\n",
    "\n",
    "    # check if the newspaper is already preprocessed, if it is skip it\n",
    "    for newspaper in newspaper_list:\n",
    "        if f\"{newspaper}\" not in preprocessed_newspapers:\n",
    "            preprocessed_newspapers[newspaper] = {}\n",
    "            dict_newspaper = preprocessed_newspapers[newspaper]\n",
    "\n",
    "            article_list = create_article_list(extracted_file, newspaper)\n",
    "            sentence_list = preprocess_newspaper(article_list)\n",
    "\n",
    "            dict_newspaper[\"no_of_articles\"] = no_of_articles(article_list)\n",
    "            dict_newspaper[\"corpus_size_before_preprocess\"] = corpus_size_before(article_list)\n",
    "            dict_newspaper[\"corpus_size\"] = corpus_size_after(sentence_list)\n",
    "            dict_newspaper[\"no_of_unique_words\"] = no_of_unique_words(sentence_list)\n",
    "            dict_newspaper[\"no_of_sentences\"] = no_of_sentences(sentence_list)\n",
    "            dict_newspaper[\"occurance_palestine\"] = occurance(\"palestine\", sentence_list)\n",
    "            dict_newspaper[\"occurance_palestinian\"] = occurance(\"palestinian\", sentence_list)\n",
    "            dict_newspaper[\"occurance_hamas\"] = occurance(\"hamas\", sentence_list)\n",
    "            dict_newspaper[\"occurance_sinwar\"] = occurance(\"sinwar\", sentence_list)\n",
    "            dict_newspaper[\"occurance_israel\"] = occurance(\"israel\", sentence_list)\n",
    "            dict_newspaper[\"occurance_israeli\"] = occurance(\"israeli\", sentence_list)\n",
    "            dict_newspaper[\"occurance_idf\"] = occurance(\"idf\", sentence_list)\n",
    "            dict_newspaper[\"occurance_netanyahu\"] = occurance(\"netanyahu\", sentence_list)\n",
    "            dict_newspaper[\"model_location\"] = \"\"\n",
    "            dict_newspaper[\"vectors_txt_location\"] = \"\"\n",
    "            dict_newspaper[\"vectors_bin_location\"] = \"\"\n",
    "            dict_newspaper[\"portrayal_palestine\"] = {}\n",
    "            dict_newspaper[\"portrayal_palestine_score\"] = 0\n",
    "            dict_newspaper[\"portrayal_israel\"] = {}\n",
    "            dict_newspaper[\"portrayal_israel_score\"] = 0\n",
    "            dict_newspaper[\"palestine-israel_score\"] = 0\n",
    "            dict_newspaper[\"articles\"] = article_list\n",
    "            dict_newspaper[\"preprocessed\"] = sentence_list\n",
    "\n",
    "            # actually fill out the values for model-related keys\n",
    "            dict_newspaper[\"model_location\"], dict_newspaper[\"vectors_txt_location\"], dict_newspaper[\"vectors_bin_location\"] = train(newspaper, sentence_list)\n",
    "\n",
    "            # Load the model from a file\n",
    "            model = Word2Vec.load(f\"{newspaper}/{newspaper}_w2v.model\")\n",
    "\n",
    "\n",
    "            palestinian_words = [\"palestine\", \"palestinian\", \"hamas\", \"sinwar\"]\n",
    "            israeli_words = [\"israel\", \"israeli\", \"idf\", \"netanyahu\"]\n",
    "\n",
    "            # positive categories: general (good etc), victim, \n",
    "            positive_portrayal_words = [\"positive\", \"good\", \"victim\", \"resilient\", \"justified\", \"defend\", \"innocent\", \"rightful\", \"humane\"]\n",
    "            negative_portrayal_words = [\"negative\", \"bad\", \"aggressor\", \"attacker\", \"brutal\", \"illegal\", \"terrorist\", \"barbaric\", \"massacre\", \"invade\"]\n",
    "\n",
    "            dict_newspaper[\"portrayal_palestine\"], dict_newspaper[\"portrayal_israel\"] = calculate_portrayal(model,  palestinian_words, israeli_words, positive_portrayal_words, negative_portrayal_words)\n",
    "            print(f\"{newspaper}\", dict_newspaper[\"portrayal_palestine\"], dict_newspaper[\"portrayal_israel\"])\n",
    "\n",
    "            for key, value in dict_newspaper[\"portrayal_palestine\"].items():\n",
    "                dict_newspaper[\"portrayal_palestine_score\"] += (value/4)  # divide by four to get the average\n",
    "\n",
    "            for key, value in dict_newspaper[\"portrayal_israel\"].items():\n",
    "                dict_newspaper[\"portrayal_israel_score\"] += (value/4)\n",
    "\n",
    "            dict_newspaper[\"palestine-israel_score\"] = dict_newspaper[\"portrayal_palestine_score\"] - dict_newspaper[\"portrayal_israel_score\"]\n",
    "            print(\"palestinian are better portrayed by: \", dict_newspaper[\"palestine-israel_score\"])\n",
    "\n",
    "            save_newspaper_dict(preprocessed_newspapers)\n",
    "            \n",
    "    return preprocessed_newspapers"
   ],
   "id": "aa6c3f9e44a3a942",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "processed_newspapers = master(\"data/news-data-extracted.json\", newspaper_list)",
   "id": "3c806a0f23fd416e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
