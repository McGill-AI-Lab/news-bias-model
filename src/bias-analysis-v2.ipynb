{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eaba59c3bf0d827",
   "metadata": {},
   "source": [
    "# Newspaper portrayal analysis of Israel and Palestine\n",
    "In this project, we train a word embedding model (a model that can assign meaningful vectors to words), specifically Word2Vec, on multiples newspapers' corpora. We create these corpora by scraping websites of different sources. If you would like to see how we scraped, please check out the github repository for the project: https://github.com/McGill-AI-Lab/news-bias-model\n",
    "\n",
    "#### Note:\n",
    "In the following jupyter notebook, we had to delete some of the cell outputs due to either them being too large, or some copyright constraints. Please know that we ran all of the code in the notebook.\n",
    "\n",
    "### Abstract\n",
    "lorem ipsum\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Access and preprocess our data",
   "id": "8de74c7d7aa56c9d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We access our data in 'data/news-data-extracted.json', which you should also have access to through the repository. This file is a dictionary, with keys corresponding to different newspapers, and for each newspaper key, the corresponding value is a list of dictionaries, each dictionary containing key-value pairs for a single article. Keys include: url, title, authors, date, text",
   "id": "990190670bee783b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T14:16:41.943375Z",
     "start_time": "2024-11-29T14:16:41.659825Z"
    }
   },
   "source": [
    "import json\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open('data/news-data-extracted.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Print the data\n",
    "first_article_data = data[\"cnn.com\"][0] #cnn is the key to a value which is a list of dictionaries, we get the first dictionary (article) of that list of dictionary\n",
    "first_article = first_article_data[\"text\"]\n",
    "print(first_article_data)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://www.cnn.com/2020/01/23/opinions/auschwitz-anniversary-anti-semitism-fears-linger-andelman/index.html', 'title': 'World leaders in Jerusalem show battle against anti-Semitism not yet a victory (opinion)', 'authors': ['David A. Andelman'], 'date': '2020-01-23 00:00:00', 'text': 'Editors Note: David A. Andelman, Executive Director of The RedLines Project, is a contributor to CNN where his columns won the Deadline Club Award for Best Opinion Writing. Author of A Shattered Peace: Versailles 1919 and the Price We Pay Today, and the forthcoming A Red Line in the Sand: Diplomacy, Strategy and a History of Wars That Almost Happened, he was formerly a foreign correspondent for The New York Times and CBS News in Europe and Asia. Follow him on Twitter @DavidAndelman. The views expressed in this commentary are his own. View more opinion on CNN.CNN The world converged on Jerusalem this week to observe the 75th anniversary of the liberation of the Auschwitz death camp  and with a collective determination to battle anti-Semitism in its many forms. At the same time, the gathering exposed a number of old and festering political wounds, which threatened to weaken the impact of it.Some 40 heads of state or government from Russian President Vladimir Putin to French President Emmanuel Macron traveled to Israels Yad Vashem Holocaust memorial to pay homage to those who died and to pledge never again. The actual anniversary of the liberation of Auschwitz, where at least 1.1 million were slaughtered in the Nazi death camp, will be marked on Monday in Poland.In Israel, though, there were noticeable absentees and a succession of contretemps.Poland and Lithuania, for example, have accused Putin of reviving Stalinist-era tropes when hes claimed the Soviet Union saved the world from Nazism and framed the Polish people as participating in the Holocaust. So, when Putin was given a speaking role at Yad Vashem, Presidents Andrzej Duda of Poland and Lithuanias Gitanas Nauseda declined to attend.Meanwhile, French President Emmanuel Macron got into a minor shouting match with Israeli police, who sought to enter the medieval Church of Saint Anne in Jerusalem  French territory since the 19th century  ahead of the French leader. (Notably, a nearly identical incident happened with the late French President Jacques Chirac in 1996).Later, however, during a group photo of the world leaders at the home of Israeli President Reuven Rivlin, Macron was strategically pictured front and center between hosts Rivlin and Prime Minister Benjamin Netanyahu.In a fight for his own political life, under indictment for corruption and facing new elections in March, Netanyahu has been making the most of this weeks events. He even leaned into his role as statesman, strolling for television cameras with Putin and then taking him on a visit to the mother of Naama Issachar, an Israeli-American woman imprisoned in Russia.This awkward moment as leader of a country keeping a womans child in prison was just one of the issues facing Putin at the ceremonies. He must decide whether to free Issachar, who is serving seven and a half years in prison after marijuana was found in her luggage while she transited a Moscow airport. (Issachar maintains the drugs were planted on her, and she was forced into signing a false confession.)Israeli media suggested Putin was demanding a number of gestures from Israel before Putin agrees to release her, a decision which seems increasingly likely after the high-octane campaign on her behalf.And, in Paris, Macron is facing demands, reiterated by Netanyahu during their meeting, to deal with the murder of Sarah Halimi, a 65-year-old French Jew who was thrown from her Paris apartment window in 2017. A French court ruled that the killer was not criminally responsible. Despite the court ruling, many protesters in France believe the killing of Halimi to be an anti-Semitic act.Meanwhile, Macron  as well as leaders across the globe  have a broader range of issues to address regarding growing anti-Semitism. The Conference on Jewish Material Claims Against Germany, a nearly 60-year-old organization dealing with Holocaust education, released a survey this week showing that 57% of all French adults and 69% of millennials and Generation Z did not know 6 million Jews were killed in the Holocaust.Get our weekly newsletter Sign up for CNN Opinions new newsletter. Join us on Twitter and FacebookMany members of the Jewish community question whether world leaders were prepared to demonstrate, as King Felipe VI of Spain said, an unyielding commitment to fighting the ignorant intolerance, hatred and the total lack of human empathy that permitted and gave birth to the Holocaust.The skepticism remains because despite wide-ranging efforts by well-meaning world leaders, anti-Semitic incidents and attacks on Jews and Jewish establishments are on the rise across Europe and the United StatesDefining and then dealing with anti-Semitism remain two critical hurdles that even the best-intentioned officials seem ill-equipped to handle. But, as King Felipe concluded, There is no room for indifference in the presence of racism, xenophobia, hate speech and anti-Semitism.'}\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "1d65473b172ae9cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T14:16:41.951297Z",
     "start_time": "2024-11-29T14:16:41.943375Z"
    }
   },
   "source": [
    "first_article[0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We need to preprocess our data. Preprocessing includes dividing articles into sentences using nltk library, since Word2Vec is trained by using list of words (sentences). Nltk uses a machine learning model to decide how to divide an article into sentences, so there will be some inaccuracies, however we can ignore these. After, we want all words to be lowercase. We want to remove all extremely high-frequency words which don't really contribute to any of the word embeddings for other words they co-occur with as these high-frequency word co-occur with a big portion of our corpus. These words are called \"stop words\" and some example would be \"I\", \"you\", \"of\", \"there\" etc. Then, we lemmatize words, i.e. try to convert each word to their root (running -> run). The goal of this process is that so we have more information about the word \"run\", instead of the information being distributed between various forms of the word (\"runs\", \"running\", \"ran\"). For more information on lemmatizers: https://www.geeksforgeeks.org/python-lemmatization-with-nltk/. Finally, we remove punctuation and put all of these functions in one \"preprocess\" function.",
   "id": "bac91c26f330c94b"
  },
  {
   "cell_type": "code",
   "id": "309ac0a633e6010e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T14:16:43.796315Z",
     "start_time": "2024-11-29T14:16:41.951297Z"
    }
   },
   "source": [
    "from gensim.utils import tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# tokenizes article into sentences, which are also tokenized into words\n",
    "def tokenize_article(article):\n",
    "    tokenized_article = []\n",
    "    sentences  = sent_tokenize(article, language=\"english\") # divide article into sentences\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokenized_sentence = tokenize(sentence) # divide sentences into words\n",
    "        tokenized_article.append(tokenized_sentence) \n",
    "    return tokenized_article\n",
    "\n",
    "# makes each word lowercase\n",
    "def lowercase(tokenized_article):\n",
    "    lowercase_article = []\n",
    "\n",
    "    for sentence in tokenized_article:\n",
    "        current_sentence = []\n",
    "        for word in sentence:\n",
    "            current_sentence.append(word.lower())\n",
    "        lowercase_article.append(current_sentence)\n",
    "\n",
    "    return lowercase_article\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(tokenized_article):\n",
    "    # Iterate over the index and content of each sentence\n",
    "    for i in range(len(tokenized_article)):\n",
    "        # Create a new list for the filtered sentence\n",
    "        filtered_sentence = []\n",
    "        for word in tokenized_article[i]:\n",
    "            if word not in stop_words:\n",
    "                filtered_sentence.append(word)\n",
    "        # Replace the original sentence with the filtered sentence\n",
    "        tokenized_article[i] = filtered_sentence\n",
    "    return tokenized_article\n",
    "\n",
    "def lammetization(tokenized_article):\n",
    "    lammetizer = WordNetLemmatizer()\n",
    "\n",
    "    lammetized_article = []\n",
    "\n",
    "    for sentence in tokenized_article:\n",
    "        current_sentence = []\n",
    "        for word in sentence:\n",
    "            current_sentence.append(lammetizer.lemmatize(word))\n",
    "        lammetized_article.append(current_sentence)\n",
    "\n",
    "    return lammetized_article\n",
    "\n",
    "\n",
    "def remove_punctuation(tokenized_article):\n",
    "    punc_removed_article = []\n",
    "\n",
    "    for sentence in tokenized_article:\n",
    "        punc_removed_sentence = []\n",
    "        for word in sentence:\n",
    "            # Split by punctuation, filter out empty strings, and join back if needed\n",
    "            split_word = ''.join(re.split(r\"[^\\w]+\", word))\n",
    "            if split_word:  # Add non-empty words only\n",
    "                punc_removed_sentence.append(split_word)\n",
    "\n",
    "        punc_removed_article.append(punc_removed_sentence)\n",
    "\n",
    "    return punc_removed_article\n",
    "\n",
    "def preprocess_article(article):\n",
    "    t_article = tokenize_article(article)\n",
    "    l_article = lowercase(t_article)\n",
    "    r_article = remove_stopwords(l_article)\n",
    "    la_article = lammetization(r_article)\n",
    "    re_article = remove_punctuation(la_article)\n",
    "    return re_article"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T14:16:43.801305Z",
     "start_time": "2024-11-29T14:16:43.796315Z"
    }
   },
   "cell_type": "code",
   "source": "print(stop_words)",
   "id": "6278a1090040ea31",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'whom', 'doing', \"should've\", 'only', 'his', \"she's\", \"aren't\", 'the', 'which', 'they', 'because', \"haven't\", 'weren', 'but', 'there', 'were', 'has', 'should', \"hadn't\", 'do', 'after', 'ourselves', 'myself', 'this', 'was', 'shan', 'few', 'having', \"shouldn't\", 'am', 'i', 'me', 'nor', 'or', 'about', 'won', 'shouldn', 'hasn', \"you're\", 'no', \"mustn't\", \"wouldn't\", 'aren', 'him', 'have', 'from', 'in', 'hers', 'yourselves', 'above', 'and', \"couldn't\", 'through', 'them', 'same', 'y', 'against', 've', 'hadn', 'just', 'with', 'at', 'what', 'themselves', 'their', \"you'd\", 'its', 'under', 'wasn', 'other', 'not', \"you've\", 'those', 'now', 'ain', 'couldn', 'once', 'any', 'how', 'as', 'both', 'don', \"didn't\", 'd', 'is', 'all', 're', 'we', 'if', 'had', 'until', 'too', 'isn', 'own', 'himself', 'off', 'herself', \"doesn't\", \"needn't\", 'be', \"it's\", \"hasn't\", \"wasn't\", 'down', 'does', 'than', 'ours', 'to', 'on', 'our', 'here', 'been', \"won't\", 'very', 'being', 's', 'theirs', 'it', 'he', 'into', \"shan't\", 'further', 'such', 'll', 'below', 'by', 'between', 'didn', 'doesn', \"mightn't\", 'most', 'yourself', 'again', 'during', 'why', 'my', \"don't\", 'an', 'needn', 'over', 'will', 'itself', 'out', \"weren't\", 'mustn', 'haven', 'her', 'before', 'while', 'some', 'then', 'wouldn', 'your', 'm', 'who', 'these', 'for', 'more', \"isn't\", 'up', 'where', 'when', 'mightn', \"that'll\", 't', 'you', 'that', 'did', 'can', 'o', 'ma', 'she', 'yours', 'so', \"you'll\", 'are', 'each', 'a', 'of'}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "e6a5c2a3d799cc88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T14:16:45.700744Z",
     "start_time": "2024-11-29T14:16:43.801305Z"
    }
   },
   "source": [
    "preprocessed = preprocess_article(first_article)\n",
    "print(preprocessed) # this is how a preprocessed article looks like"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['editor', 'note', 'david', 'andelman', 'executive', 'director', 'redlines', 'project', 'contributor', 'cnn', 'column', 'deadline', 'club', 'award', 'best', 'opinion', 'writing'], ['author', 'shattered', 'peace', 'versailles', 'price', 'pay', 'today', 'forthcoming', 'red', 'line', 'sand', 'diplomacy', 'strategy', 'history', 'war', 'almost', 'happened', 'formerly', 'foreign', 'correspondent', 'new', 'york', 'time', 'cbs', 'news', 'europe', 'asia'], ['follow', 'twitter', 'davidandelman'], ['view', 'expressed', 'commentary'], ['view', 'opinion', 'cnn', 'cnn', 'world', 'converged', 'jerusalem', 'week', 'observe', 'th', 'anniversary', 'liberation', 'auschwitz', 'death', 'camp', 'collective', 'determination', 'battle', 'anti', 'semitism', 'many', 'form'], ['time', 'gathering', 'exposed', 'number', 'old', 'festering', 'political', 'wound', 'threatened', 'weaken', 'impact', 'head', 'state', 'government', 'russian', 'president', 'vladimir', 'putin', 'french', 'president', 'emmanuel', 'macron', 'traveled', 'israel', 'yad', 'vashem', 'holocaust', 'memorial', 'pay', 'homage', 'died', 'pledge', 'never'], ['actual', 'anniversary', 'liberation', 'auschwitz', 'least', 'million', 'slaughtered', 'nazi', 'death', 'camp', 'marked', 'monday', 'poland', 'israel', 'though', 'noticeable', 'absentee', 'succession', 'contretemps', 'poland', 'lithuania', 'example', 'accused', 'putin', 'reviving', 'stalinist', 'era', 'trope', 'he', 'claimed', 'soviet', 'union', 'saved', 'world', 'nazism', 'framed', 'polish', 'people', 'participating', 'holocaust'], ['putin', 'given', 'speaking', 'role', 'yad', 'vashem', 'president', 'andrzej', 'duda', 'poland', 'lithuania', 'gitana', 'nauseda', 'declined', 'attend', 'meanwhile', 'french', 'president', 'emmanuel', 'macron', 'got', 'minor', 'shouting', 'match', 'israeli', 'police', 'sought', 'enter', 'medieval', 'church', 'saint', 'anne', 'jerusalem', 'french', 'territory', 'since', 'th', 'century', 'ahead', 'french', 'leader'], ['notably', 'nearly', 'identical', 'incident', 'happened', 'late', 'french', 'president', 'jacques', 'chirac', 'later', 'however', 'group', 'photo', 'world', 'leader', 'home', 'israeli', 'president', 'reuven', 'rivlin', 'macron', 'strategically', 'pictured', 'front', 'center', 'host', 'rivlin', 'prime', 'minister', 'benjamin', 'netanyahu', 'fight', 'political', 'life', 'indictment', 'corruption', 'facing', 'new', 'election', 'march', 'netanyahu', 'making', 'week', 'event'], ['even', 'leaned', 'role', 'statesman', 'strolling', 'television', 'camera', 'putin', 'taking', 'visit', 'mother', 'naama', 'issachar', 'israeli', 'american', 'woman', 'imprisoned', 'russia', 'awkward', 'moment', 'leader', 'country', 'keeping', 'woman', 'child', 'prison', 'one', 'issue', 'facing', 'putin', 'ceremony'], ['must', 'decide', 'whether', 'free', 'issachar', 'serving', 'seven', 'half', 'year', 'prison', 'marijuana', 'found', 'luggage', 'transited', 'moscow', 'airport'], ['issachar', 'maintains', 'drug', 'planted', 'forced', 'signing', 'false', 'confession'], ['israeli', 'medium', 'suggested', 'putin', 'demanding', 'number', 'gesture', 'israel', 'putin', 'agrees', 'release', 'decision', 'seems', 'increasingly', 'likely', 'high', 'octane', 'campaign', 'behalf', 'paris', 'macron', 'facing', 'demand', 'reiterated', 'netanyahu', 'meeting', 'deal', 'murder', 'sarah', 'halimi', 'year', 'old', 'french', 'jew', 'thrown', 'paris', 'apartment', 'window'], ['french', 'court', 'ruled', 'killer', 'criminally', 'responsible'], ['despite', 'court', 'ruling', 'many', 'protester', 'france', 'believe', 'killing', 'halimi', 'anti', 'semitic', 'act', 'meanwhile', 'macron', 'well', 'leader', 'across', 'globe', 'broader', 'range', 'issue', 'address', 'regarding', 'growing', 'anti', 'semitism'], ['conference', 'jewish', 'material', 'claim', 'germany', 'nearly', 'year', 'old', 'organization', 'dealing', 'holocaust', 'education', 'released', 'survey', 'week', 'showing', 'french', 'adult', 'millennials', 'generation', 'z', 'know', 'million', 'jew', 'killed', 'holocaust', 'get', 'weekly', 'newsletter', 'sign', 'cnn', 'opinion', 'new', 'newsletter'], ['join', 'u', 'twitter', 'facebookmany', 'member', 'jewish', 'community', 'question', 'whether', 'world', 'leader', 'prepared', 'demonstrate', 'king', 'felipe', 'vi', 'spain', 'said', 'unyielding', 'commitment', 'fighting', 'ignorant', 'intolerance', 'hatred', 'total', 'lack', 'human', 'empathy', 'permitted', 'gave', 'birth', 'holocaust', 'skepticism', 'remains', 'despite', 'wide', 'ranging', 'effort', 'well', 'meaning', 'world', 'leader', 'anti', 'semitic', 'incident', 'attack', 'jew', 'jewish', 'establishment', 'rise', 'across', 'europe', 'united', 'statesdefining', 'dealing', 'anti', 'semitism', 'remain', 'two', 'critical', 'hurdle', 'even', 'best', 'intentioned', 'official', 'seem', 'ill', 'equipped', 'handle'], ['king', 'felipe', 'concluded', 'room', 'indifference', 'presence', 'racism', 'xenophobia', 'hate', 'speech', 'anti', 'semitism']]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def create_article_list(extracted_file, newspaper_name):\n",
    "    \"\"\"\n",
    "    Takes in the file of extracted news and the newspaper name.\n",
    "    Outputs an article (text) list for the given newspaper.\n",
    "    \"\"\"\n",
    "    import json\n",
    "\n",
    "    # Load the JSON file\n",
    "    with open(extracted_file, \"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    # Extract newspaper data\n",
    "    newspaper = data.get(newspaper_name, [])  # Default to an empty list if not found\n",
    "    newspaper_articles = []\n",
    "\n",
    "    # Loop through articles in the newspaper\n",
    "    for article in newspaper:\n",
    "        # Check if article has a valid \"text\" key\n",
    "        if article and isinstance(article, dict) and \"text\" in article:\n",
    "            newspaper_articles.append(article[\"text\"])  # Use append to add the text to the list\n",
    "\n",
    "    print(f\"Extracted {len(newspaper_articles)} articles from {newspaper_name}.\")\n",
    "    return newspaper_articles\n",
    "\n"
   ],
   "id": "953133fe35df9b92",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "94847e14cc9be247",
   "metadata": {},
   "source": [
    "def preprocess_newspaper(article_list):\n",
    "    \"\"\" Takes in article list and give back a list of list which is preprocessed article in the form of every element in the list is a sentence which consist of lists of words\"\"\"\n",
    "\n",
    "    if not article_list:  # Handle empty or None input\n",
    "        print(\"No articles provided for preprocessing.\")\n",
    "        return []\n",
    "\n",
    "    preprocessed_article_list = []\n",
    "    i = 0 # to see how many articles we processed\n",
    "\n",
    "    for article in article_list:\n",
    "        preprocessed_article_list.extend(preprocess_article(article))  # extends preproccessed\n",
    "        # articles to newspaper's article list\n",
    "        print(f\"article {i} preprocessed\")\n",
    "        i += 1\n",
    "\n",
    "    return preprocessed_article_list"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "53184256486a4c43",
   "metadata": {},
   "source": [
    "# Lets try with CNN\n",
    "article_list = create_article_list(\"data/news-data-extracted.json\", \"cnn.com\")\n",
    "print(article_list)\n",
    "preprocessed = preprocess_newspaper(article_list)\n",
    "print(preprocessed)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# create get_article_number, corpussize, and other helper functions\n",
    "\n",
    "def no_of_articles(article_list):\n",
    "    return len(article_list)\n",
    "\n",
    "def corpus_size_before(article_list):\n",
    "    corpus = article_list\n",
    "\n",
    "    corpus_size = 0\n",
    "    for article in article_list:\n",
    "        corpus_size += len(article.split())\n",
    "\n",
    "    return corpus_size\n",
    "\n",
    "\n",
    "def corpus_size_after(preprocessed_article_list):\n",
    "    corpus = preprocessed_article_list\n",
    "\n",
    "    corpus_size = 0\n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            corpus_size += 1\n",
    "\n",
    "    return corpus_size\n",
    "\n",
    "\n",
    "def no_of_unique_words(preprocessed_article_list):\n",
    "    words = []\n",
    "\n",
    "    for sentence in preprocessed_article_list:\n",
    "        for word in sentence:\n",
    "            if word in words:\n",
    "                pass\n",
    "            else:\n",
    "                words.append(word)\n",
    "\n",
    "    return len(words)\n",
    "\n",
    "def no_of_sentences(preprocessed_article_list):\n",
    "    return len(preprocessed_article_list)"
   ],
   "id": "8430fb4a02dc172c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\" Occurance Counter \"\"\"\n",
    "\n",
    "# Palestine\n",
    "def occurance(target_word, preprocessed_article_list):\n",
    "    counter = 0\n",
    "\n",
    "    for sentence in preprocessed_article_list:\n",
    "        for word in sentence:\n",
    "            if word == f\"{target_word}\":\n",
    "                counter += 1\n",
    "    \n",
    "    return counter"
   ],
   "id": "c14e2862eda84853",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training function",
   "id": "8d5de139cac5f3af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train(newspaper_name, sentence_list):\n",
    "    from gensim.models import Word2Vec\n",
    "    import os\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(newspaper_name, exist_ok=True)\n",
    "\n",
    "    # Train Word2Vec model\n",
    "    # Initialize the model with parameters\n",
    "    model = Word2Vec(sentences=sentence_list, vector_size=300, window=5, min_count=10, sg=1, workers=4, negative=20)\n",
    "\n",
    "    # Train and save the model\n",
    "    model.train(sentence_list, total_examples=len(sentence_list), epochs=20)\n",
    "    model.save(os.path.join(newspaper_name, f\"{newspaper_name}_w2v.model\"))\n",
    "\n",
    "    # # Save just the word vectors in a text and binaryformat\n",
    "    # model.wv.save_word2vec_format(f\"{newspaper_name/newspaper_name}_w2v_vectors.txt\", binary=False)\n",
    "    # model.wv.save_word2vec_format(f\"{newspaper_name/newspaper_name}_w2v_vectors.bin\", binary=True)\n",
    "\n",
    "\n",
    "    import os\n",
    "\n",
    "    model.wv.save_word2vec_format(os.path.join(newspaper_name, f\"{newspaper_name}_w2v_vectors.txt\"),binary=False)\n",
    "    model.wv.save_word2vec_format(os.path.join(newspaper_name, f\"{newspaper_name}_w2v_vectors.bin\"),binary=True)\n",
    "\n",
    "\n",
    "    return (\n",
    "        os.path.join(newspaper_name, f\"{newspaper_name}_w2v.model\"),\n",
    "        os.path.join(newspaper_name, f\"{newspaper_name}_w2v_vectors.txt\"),\n",
    "        os.path.join(newspaper_name, f\"{newspaper_name}_w2v_vectors.bin\"),\n",
    "    )\n"
   ],
   "id": "6255942a449e9d4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Calculate portrayal",
   "id": "85f72616c7809892"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_portrayal(model, palestinian_words, israeli_words, positive_portrayal_words, negative_portrayal_words): # target_words and portrayal_words are lists\n",
    "    palestine_portrayal_scores = {}\n",
    "    israel_portrayal_scores = {}\n",
    "\n",
    "    # Access the list of words in the vocabulary\n",
    "    vocabulary_words = list(model.wv.key_to_index.keys())\n",
    "    \n",
    "    # no of portrayal words\n",
    "    pos_count = 0\n",
    "    for word in positive_portrayal_words:\n",
    "        if word in vocabulary_words:\n",
    "            pos_count += 1\n",
    "    neg_count = 0\n",
    "    for word in negative_portrayal_words:\n",
    "        if word in vocabulary_words:\n",
    "            neg_count += 1       \n",
    "\n",
    "\n",
    "    for word in palestinian_words:\n",
    "            palestine_portrayal_scores[word] = 0\n",
    "            for positive in positive_portrayal_words:\n",
    "                if positive in vocabulary_words:\n",
    "                    palestine_portrayal_scores[word] += (model.wv.similarity(f\"{word}\", f\"{positive}\")/pos_count)\n",
    "            for negative in negative_portrayal_words:\n",
    "                if positive in vocabulary_words:\n",
    "                    palestine_portrayal_scores[word] -= (model.wv.similarity(f\"{word}\", f\"{negative}\")/neg_count)\n",
    "\n",
    "    for word in israeli_words:\n",
    "        israel_portrayal_scores[word] = 0\n",
    "        for positive in positive_portrayal_words:\n",
    "            if positive in vocabulary_words:\n",
    "                israel_portrayal_scores[word] += (model.wv.similarity(f\"{word}\", f\"{positive}\")/pos_count)\n",
    "        for negative in negative_portrayal_words:\n",
    "            if positive in vocabulary_words:\n",
    "                israel_portrayal_scores[word] -= (model.wv.similarity(f\"{word}\", f\"{negative}\")/neg_count)\n",
    "\n",
    "    return palestine_portrayal_scores, israel_portrayal_scores"
   ],
   "id": "6cd127c8489a891d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Should I include gaza, if yes, add an occurance function and add it to the target word_list and portrayal",
   "id": "57b27bf3467fde6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Save newspaper dictionary",
   "id": "3a0781d7cf229880"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def save_newspaper_dict(newspaper_dict):\n",
    "    # File path for the JSON file\n",
    "    file_path = \"preprocessed_newspapers_dict.json\"\n",
    "\n",
    "    # Open the JSON file\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        data = json.load(json_file)  # Load existing data\n",
    "\n",
    "    # Iterate over items in the dictionary\n",
    "    for key, value in newspaper_dict.items():  # Use .items() to get key-value pairs\n",
    "        if key not in data:\n",
    "            data[key] = value  # Save new key-value pair\n",
    "\n",
    "    # Save updated data back to the file\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(data, json_file, indent=4)"
   ],
   "id": "82993f5040b960cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def load_preprocessed_newspapers(json_file):\n",
    "    \"\"\"\n",
    "    Load preprocessed newspapers from a JSON file.\n",
    "    \"\"\"\n",
    "    if os.path.exists(json_file):\n",
    "        try:\n",
    "            with open(json_file, 'r') as file:\n",
    "                data = json.load(file)\n",
    "                if isinstance(data, dict):\n",
    "                    print(f\"Successfully loaded preprocessed newspapers from {json_file}.\")\n",
    "                    return data\n",
    "                else:\n",
    "                    print(\"Error: JSON data is not a dictionary. Returning an empty dictionary.\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON file {json_file}: {e}\")\n",
    "    else:\n",
    "        print(f\"File {json_file} does not exist. Starting with an empty dictionary.\")\n",
    "\n",
    "    return {}\n"
   ],
   "id": "f1ee0f4d346df856",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "newspaper_list = [\"cnn.com\", \"WashingtonPost.com\"]\n",
    "\n",
    "def master(extracted_file, newspaper_list):\n",
    "    \"\"\"\n",
    "    Get a list of newspapers\n",
    "    Create a dictionary of newspapers, which is a dictionary\n",
    "    For every newspaper, have the following keys:\n",
    "    # of articles, corpus size (before preprocessing), # of unique words (before preprocessing),\n",
    "    list of articles (only text) (before preprocessing),\n",
    "    preprocessed articles (a list of sentences, which are a list of words)\n",
    "    corpus size (after preprocessing), # of unique words (after preprocessing),\n",
    "    how many times each target word appears (palestine, israel, hamas, idf, netanyahu, sinwar, etc.)\n",
    "    train a word2vec, save the model and the weights,\n",
    "    bias score for palestine, israel, hamas, idf, etc,\n",
    "    Add the following key to each articles\n",
    "    \"\"\"\n",
    "    from gensim.models import KeyedVectors\n",
    "    from gensim.models import Word2Vec\n",
    "\n",
    "    preprocessed_newspapers = load_preprocessed_newspapers(\"preprocessed_newspapers_dict.json\")\n",
    "\n",
    "\n",
    "    # check if the newspaper is already preprocessed, if it is skip it\n",
    "    for newspaper in newspaper_list:\n",
    "        if f\"{newspaper}\" not in preprocessed_newspapers:\n",
    "            preprocessed_newspapers[newspaper] = {}\n",
    "            dict_newspaper = preprocessed_newspapers[newspaper]\n",
    "\n",
    "            article_list = create_article_list(extracted_file, newspaper)\n",
    "            sentence_list = preprocess_newspaper(article_list)\n",
    "\n",
    "            dict_newspaper[\"no_of_articles\"] = no_of_articles(article_list)\n",
    "            dict_newspaper[\"corpus_size_before_preprocess\"] = corpus_size_before(article_list)\n",
    "            dict_newspaper[\"corpus_size\"] = corpus_size_after(sentence_list)\n",
    "            dict_newspaper[\"no_of_unique_words\"] = no_of_unique_words(sentence_list)\n",
    "            dict_newspaper[\"no_of_sentences\"] = no_of_sentences(sentence_list)\n",
    "            dict_newspaper[\"occurance_palestine\"] = occurance(\"palestine\", sentence_list)\n",
    "            dict_newspaper[\"occurance_palestinian\"] = occurance(\"palestinian\", sentence_list)\n",
    "            dict_newspaper[\"occurance_hamas\"] = occurance(\"hamas\", sentence_list)\n",
    "            dict_newspaper[\"occurance_sinwar\"] = occurance(\"sinwar\", sentence_list)\n",
    "            dict_newspaper[\"occurance_israel\"] = occurance(\"israel\", sentence_list)\n",
    "            dict_newspaper[\"occurance_israeli\"] = occurance(\"israeli\", sentence_list)\n",
    "            dict_newspaper[\"occurance_idf\"] = occurance(\"idf\", sentence_list)\n",
    "            dict_newspaper[\"occurance_netanyahu\"] = occurance(\"netanyahu\", sentence_list)\n",
    "            dict_newspaper[\"model_location\"] = \"\"\n",
    "            dict_newspaper[\"vectors_txt_location\"] = \"\"\n",
    "            dict_newspaper[\"vectors_bin_location\"] = \"\"\n",
    "            dict_newspaper[\"portrayal_palestine\"] = {}\n",
    "            dict_newspaper[\"portrayal_palestine_score\"] = 0\n",
    "            dict_newspaper[\"portrayal_israel\"] = {}\n",
    "            dict_newspaper[\"portrayal_israel_score\"] = 0\n",
    "            dict_newspaper[\"palestine-israel_score\"] = 0\n",
    "            dict_newspaper[\"articles\"] = article_list\n",
    "            dict_newspaper[\"preprocessed\"] = sentence_list\n",
    "\n",
    "            # actually fill out the values for model-related keys\n",
    "            dict_newspaper[\"model_location\"], dict_newspaper[\"vectors_txt_location\"], dict_newspaper[\"vectors_bin_location\"] = train(newspaper, sentence_list)\n",
    "\n",
    "            # Load the model from a file\n",
    "            model = Word2Vec.load(f\"{newspaper}/{newspaper}_w2v.model\")\n",
    "\n",
    "\n",
    "            palestinian_words = [\"palestine\", \"palestinian\", \"hamas\", \"sinwar\"]\n",
    "            israeli_words = [\"israel\", \"israeli\", \"idf\", \"netanyahu\"]\n",
    "\n",
    "            # positive categories: general (good etc), victim, \n",
    "            positive_portrayal_words = [\"positive\", \"good\", \"victim\", \"resilient\", \"justified\", \"defend\", \"innocent\", \"rightful\", \"humane\"]\n",
    "            negative_portrayal_words = [\"negative\", \"bad\", \"aggressor\", \"attacker\", \"brutal\", \"illegal\", \"terrorist\", \"barbaric\", \"massacre\", \"invade\"]\n",
    "\n",
    "            dict_newspaper[\"portrayal_palestine\"], dict_newspaper[\"portrayal_israel\"] = calculate_portrayal(model,  palestinian_words, israeli_words, positive_portrayal_words, negative_portrayal_words)\n",
    "            print(f\"{newspaper}\", dict_newspaper[\"portrayal_palestine\"], dict_newspaper[\"portrayal_israel\"])\n",
    "\n",
    "            for key, value in dict_newspaper[\"portrayal_palestine\"].items():\n",
    "                dict_newspaper[\"portrayal_palestine_score\"] += (value/4)  # divide by four to get the average\n",
    "\n",
    "            for key, value in dict_newspaper[\"portrayal_israel\"].items():\n",
    "                dict_newspaper[\"portrayal_israel_score\"] += (value/4)\n",
    "\n",
    "            dict_newspaper[\"palestine-israel_score\"] = dict_newspaper[\"portrayal_palestine_score\"] - dict_newspaper[\"portrayal_israel_score\"]\n",
    "            print(\"palestinian are better portrayed by: \", dict_newspaper[\"palestine-israel_score\"])\n",
    "\n",
    "            save_newspaper_dict(preprocessed_newspapers)\n",
    "            \n",
    "    return preprocessed_newspapers"
   ],
   "id": "aa6c3f9e44a3a942",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "processed_newspapers = master(\"data/news-data-extracted.json\", newspaper_list)",
   "id": "3c806a0f23fd416e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
