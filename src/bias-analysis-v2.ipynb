{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eaba59c3bf0d827",
   "metadata": {},
   "source": [
    "What we need to do: \n",
    "1) Go through each newspaper and create a newspaper corpora, display the length of each corpora\n",
    "2) Find either a) wikipedia dataset and train word2vec on it to compare non-context related words like \"bad\", \n",
    "\"victim\", \"good\", \"attacker\" to representation of each side of the conflict in the newspaper corpora, b) find a \n",
    "pretrained word2vec to do the same\n",
    "3) Find a way to classify each article as pro-israel or pro-palestine"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Access and preprocess our data",
   "id": "8de74c7d7aa56c9d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open('data/news-data-extracted.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Print the data\n",
    "first_article_data = data[\"cnn.com\"][0] #cnn is the key to a value which is a list of dictionaries, we get the first dictionary (article) of that list of dictionary\n",
    "first_article = first_article_data[\"text\"]\n",
    "print(first_article_data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d65473b172ae9cd",
   "metadata": {},
   "source": [
    "first_article[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "309ac0a633e6010e",
   "metadata": {},
   "source": [
    "from gensim.utils import tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# tokenizes article into sentences, which are also tokenized into words\n",
    "def tokenize_article(article):\n",
    "    tokenized_article = []\n",
    "    sentences  = sent_tokenize(article, language=\"english\") # divide article into sentences\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokenized_sentence = tokenize(sentence) # divide sentences into words\n",
    "        tokenized_article.append(tokenized_sentence) \n",
    "    return tokenized_article\n",
    "\n",
    "# makes each word lowercase\n",
    "def lowercase(tokenized_article):\n",
    "    lowercase_article = []\n",
    "\n",
    "    for sentence in tokenized_article:\n",
    "        current_sentence = []\n",
    "        for word in sentence:\n",
    "            current_sentence.append(word.lower())\n",
    "        lowercase_article.append(current_sentence)\n",
    "\n",
    "    return lowercase_article\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(tokenized_article):\n",
    "    # Iterate over the index and content of each sentence\n",
    "    for i in range(len(tokenized_article)):\n",
    "        # Create a new list for the filtered sentence\n",
    "        filtered_sentence = []\n",
    "        for word in tokenized_article[i]:\n",
    "            if word not in stop_words:\n",
    "                filtered_sentence.append(word)\n",
    "        # Replace the original sentence with the filtered sentence\n",
    "        tokenized_article[i] = filtered_sentence\n",
    "    return tokenized_article\n",
    "\n",
    "def lammetization(tokenized_article):\n",
    "    lammetizer = WordNetLemmatizer()\n",
    "\n",
    "    lammetized_article = []\n",
    "\n",
    "    for sentence in tokenized_article:\n",
    "        current_sentence = []\n",
    "        for word in sentence:\n",
    "            current_sentence.append(lammetizer.lemmatize(word))\n",
    "        lammetized_article.append(current_sentence)\n",
    "\n",
    "    return lammetized_article\n",
    "\n",
    "\n",
    "def remove_punctuation(tokenized_article):\n",
    "    punc_removed_article = []\n",
    "\n",
    "    for sentence in tokenized_article:\n",
    "        punc_removed_sentence = []\n",
    "        for word in sentence:\n",
    "            # Split by punctuation, filter out empty strings, and join back if needed\n",
    "            split_word = ''.join(re.split(r\"[^\\w]+\", word))\n",
    "            if split_word:  # Add non-empty words only\n",
    "                punc_removed_sentence.append(split_word)\n",
    "\n",
    "        punc_removed_article.append(punc_removed_sentence)\n",
    "\n",
    "    return punc_removed_article\n",
    "\n",
    "def preprocess_article(article):\n",
    "    t_article = tokenize_article(article)\n",
    "    l_article = lowercase(t_article)\n",
    "    r_article = remove_stopwords(l_article)\n",
    "    la_article = lammetization(r_article)\n",
    "    re_article = remove_punctuation(la_article)\n",
    "    return re_article"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(stop_words)\n",
    "print(stopwords.words(\"english\"))\n",
    "print(set(stopwords.words(\"english\")))"
   ],
   "id": "6278a1090040ea31",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e6a5c2a3d799cc88",
   "metadata": {},
   "source": [
    "preprocessed = preprocess_article(first_article)\n",
    "print(preprocessed)\n",
    "for word in stopwords.words(\"english\"):\n",
    "    if word in preprocessed:\n",
    "        print(word)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "80ae5791093d845d",
   "metadata": {},
   "source": [
    "Now create a function that preperocesses a newspaper"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_article_list(extracted_file, newspaper_name):\n",
    "    \"\"\" Takes in the file of extracted news and the newspaper name\n",
    "    Outputs an artilce (text) list for the given newspaper\n",
    "    \"\"\"\n",
    "    import json\n",
    "    with open(extracted_file, \"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    newspaper = data[newspaper_name] # newspaper will be a dictionary of articles with values being url, date, authors, text etc.\n",
    "    newspaper_articles = []\n",
    "    \n",
    "    for article in newspaper:\n",
    "        newspaper_articles.append(f\"{article[\"text\"]}\")\n",
    "    \n",
    "    print(newspaper_articles)"
   ],
   "id": "953133fe35df9b92",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "94847e14cc9be247",
   "metadata": {},
   "source": [
    "def preprocess_newspaper(article_list):\n",
    "    \"\"\" Takes in article list and give back a list of list which is preprocessed article in the form of every every element in the list is a sentence which consist of lists of words\"\"\"\n",
    "    preprocessed_article_list = []\n",
    "    i = 0 # to see how many articles we processed\n",
    "\n",
    "    for article in article_list:\n",
    "        preprocessed_article_list.extend(preprocess_article(article))  # extends preproccessed\n",
    "        # articles to newspaper's article list\n",
    "        print(f\"article {i} preprocessed\")\n",
    "        i += 1\n",
    "\n",
    "    return preprocessed_article_list"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "53184256486a4c43",
   "metadata": {},
   "source": [
    "# Lets try with CNN\n",
    "article_list = create_article_list(\"data/news-data-extracted.json\", \"cnn.com\")\n",
    "preprocessed = preprocess_newspaper(article_list)\n",
    "print(preprocessed)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# create get_article_number, corpussize, and other helper functions\n",
    "\n",
    "def no_of_articles(article_list):\n",
    "    return len(article_list)\n",
    "\n",
    "def corpus_size_before(article_list):\n",
    "    corpus = article_list\n",
    "\n",
    "    corpus_size = 0\n",
    "    for article in article_list:\n",
    "        corpus_size += len(article)\n",
    "\n",
    "    return corpus_size\n",
    "\n",
    "\n",
    "def corpus_size_after(preprocessed_article_list):\n",
    "    corpus = preprocessed_article_list\n",
    "\n",
    "    corpus_size = 0\n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            corpus_size += 1\n",
    "\n",
    "    return corpus_size\n",
    "\n",
    "\n",
    "def no_of_unique_words(preprocessed_article_list):\n",
    "    words = []\n",
    "\n",
    "    for sentence in preprocessed_article_list:\n",
    "        for word in sentence:\n",
    "            if word in words:\n",
    "                pass\n",
    "            else:\n",
    "                words.append(word)\n",
    "\n",
    "    return len(words)\n",
    "\n",
    "def no_of_sentences(preprocessed_article_list):\n",
    "    return len(preprocessed_article_list)"
   ],
   "id": "8430fb4a02dc172c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\" Occurance Counter \"\"\"\n",
    "\n",
    "# Palestine\n",
    "def occurance_palestine(preprocessed_article_list):\n",
    "    counter = 0\n",
    "\n",
    "    for sentence in preprocessed_article_list:\n",
    "        for word in sentence:\n",
    "            if word == \"palestine\":\n",
    "                counter += 1\n",
    "    return counter\n",
    "\n",
    "\n",
    "def occurance_palestinian(preprocessed_article_list):\n",
    "    counter = 0\n",
    "\n",
    "    for sentence in preprocessed_article_list:\n",
    "        for word in sentence:\n",
    "            if word == \"palestinian\":\n",
    "                counter += 1\n",
    "    return counter\n",
    "\n",
    "\n",
    "def occurance_hamas(preprocessed_article_list):\n",
    "    counter = 0\n",
    "\n",
    "    for sentence in preprocessed_article_list:\n",
    "        for word in sentence:\n",
    "            if word == \"hamas\":\n",
    "                counter += 1\n",
    "    return counter\n",
    "\n",
    "\n",
    "def occurance_sinwar(preprocessed_article_list):\n",
    "    counter = 0\n",
    "\n",
    "    for sentence in preprocessed_article_list:\n",
    "        for word in sentence:\n",
    "            if word == \"sinwar\":\n",
    "                counter += 1\n",
    "    return counter\n",
    "\n",
    "\n",
    "# Israel\n",
    "def occurance_israel(preprocessed_article_list):\n",
    "    counter = 0\n",
    "\n",
    "    for sentence in preprocessed_article_list:\n",
    "        for word in sentence:\n",
    "            if word == \"israel\":\n",
    "                counter += 1\n",
    "    return counter\n",
    "\n",
    "\n",
    "def occurance_israeli(preprocessed_article_list):\n",
    "    counter = 0\n",
    "\n",
    "    for sentence in preprocessed_article_list:\n",
    "        for word in sentence:\n",
    "            if word == \"israeli\":\n",
    "                counter += 1\n",
    "    return counter\n",
    "\n",
    "\n",
    "def occurance_idf(preprocessed_article_list):\n",
    "    counter = 0\n",
    "\n",
    "    for sentence in preprocessed_article_list:\n",
    "        for word in sentence:\n",
    "            if word == \"idf\":\n",
    "                counter += 1\n",
    "    return counter\n",
    "\n",
    "\n",
    "def occurance_netanyahu(preprocessed_article_list):\n",
    "    counter = 0\n",
    "\n",
    "    for sentence in preprocessed_article_list:\n",
    "        for word in sentence:\n",
    "            if word == \"netanyahu\":\n",
    "                counter += 1\n",
    "    return counter"
   ],
   "id": "c14e2862eda84853",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training function",
   "id": "8d5de139cac5f3af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train(newspaper_name, sentence_list):\n",
    "    from gensim.models import Word2Vec\n",
    "    # Train Word2Vec model\n",
    "    # Initialize the model with parameters\n",
    "    model = Word2Vec(sentences=sentence_list, vector_size=300, window=5, min_count=10, sg=1, workers=4, negative=20)\n",
    "\n",
    "    # Train and save the model\n",
    "    model.train(sentence_list, total_examples=len(sentence_list), epochs=20)\n",
    "    model.save(f\"{newspaper_name}_w2v.model\")\n",
    "\n",
    "    # Save just the word vectors in a text and binaryformat\n",
    "    model.wv.save_word2vec_format(f\"{newspaper_name/newspaper_name}_w2v_vectors.txt\", binary=False)\n",
    "    model.wv.save_word2vec_format(f\"{newspaper_name/newspaper_name}_w2v_vectors.bin\", binary=True)\n",
    "\n",
    "    return f\"{newspaper_name}_w2v.model\", f\"{newspaper_name}_w2v_vectors.txt\", f\"{newspaper_name}_w2v_vectors.bin\"\n"
   ],
   "id": "6255942a449e9d4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Calculate portrayal",
   "id": "85f72616c7809892"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_portrayal(model, target_words, positive_portrayal_words, negative_portrayal_words): # target_words and portrayal_words are lists\n",
    "    portrayal_scores = {}\n",
    "\n",
    "    for word in target_words:\n",
    "        portrayal_scores[word] = 0\n",
    "        for positive in positive_portrayal_words:\n",
    "            portrayal_scores[word] += model.wv.similarity(f\"{word}\", f\"{positive}\")\n",
    "        for negative in negative_portrayal_words:\n",
    "            portrayal_scores[word] -= model.wv.similarity(f\"{word}\", f\"{negative}\")\n",
    "\n",
    "    return portrayal_scores\n"
   ],
   "id": "6cd127c8489a891d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Should I include gaza, if yes, add an occurance function and add it to the target word_list and portrayal",
   "id": "57b27bf3467fde6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "newspaper_list = [\"cnn.com\", \"WashingtonPost.com\"]\n",
    "\n",
    "def master(extracted_file, newspaper_list):\n",
    "    \"\"\"\n",
    "    Get a list of newspapers\n",
    "    Create a dictionary of newspapers, which is a dictionary\n",
    "    For every newspaper, have the following keys:\n",
    "    # of articles, corpus size (before preprocessing), # of unique words (before preprocessing),\n",
    "    list of articles (only text) (before preprocessing),\n",
    "    preprocessed articles (a list of sentences, which are a list of words)\n",
    "    corpus size (after preprocessing), # of unique words (after preprocessing),\n",
    "    how many times each target word appears (palestine, israel, hamas, idf, netanyahu, sinwar, etc.)\n",
    "    train a word2vec, save the model and the weights,\n",
    "    bias score for palestine, israel, hamas, idf, etc,\n",
    "    Add the following key to each articles\n",
    "    \"\"\"\n",
    "    from gensim.models import KeyedVectors\n",
    "    from gensim.models import Word2Vec\n",
    "\n",
    "    preprocessed_newspapers = {}\n",
    "\n",
    "    # check if the newspaper is already preprocessed, if it is skip it\n",
    "    for newspaper in newspaper_list:\n",
    "        if newspaper not in preprocessed_newspapers:\n",
    "            preprocessed_newspapers[newspaper] = {}\n",
    "            dict_newspaper = preprocessed_newspapers[newspaper]\n",
    "\n",
    "            article_list = create_article_list(extracted_file, newspaper)\n",
    "            sentence_list = preprocess_newspaper(article_list)\n",
    "\n",
    "            dict_newspaper[\"no_of_articles\"] = no_of_articles(article_list)\n",
    "            dict_newspaper[\"corpus_size_before_preprocess\"] = corpus_size_before(article_list)\n",
    "            dict_newspaper[\"corpus_size\"] = corpus_size_after(sentence_list)\n",
    "            dict_newspaper[\"no_of_unique_words\"] = no_of_unique_words(sentence_list)\n",
    "            dict_newspaper[\"no_of_sentences\"] = no_of_sentences(sentence_list)\n",
    "            dict_newspaper[\"occurance_palestine\"] = occurance_palestine(sentence_list)\n",
    "            dict_newspaper[\"occurance_palestinian\"] = occurance_palestinian(sentence_list)\n",
    "            dict_newspaper[\"occurance_hamas\"] = occurance_hamas(sentence_list)\n",
    "            dict_newspaper[\"occurance_sinwar\"] = occurance_sinwar(sentence_list)\n",
    "            dict_newspaper[\"occurance_israel\"] = occurance_israel(sentence_list)\n",
    "            dict_newspaper[\"occurance_israeli\"] = occurance_israeli(sentence_list)\n",
    "            dict_newspaper[\"occurance_idf\"] = occurance_idf(sentence_list)\n",
    "            dict_newspaper[\"occurance_netanyahu\"] = occurance_netanyahu(sentence_list)\n",
    "            dict_newspaper[\"model_location\"] = \"\"\n",
    "            dict_newspaper[\"vectors_txt_location\"] = \"\"\n",
    "            dict_newspaper[\"vectors_bin_location\"] = \"\"\n",
    "            # dict_newspaper[\"portrayal_palestine\"] = portrayal_palestine(sentence_list)\n",
    "            # dict_newspaper[\"portrayal_palestinian\"] = portrayal_palestinian(sentence_list)\n",
    "            # dict_newspaper[\"portrayal_hamas\"] = portrayal_hamas(sentence_list)\n",
    "            # dict_newspaper[\"portrayal_sinwar\"] = portrayal_sinwar(sentence_list)\n",
    "            # dict_newspaper[\"portrayal_israel\"] = portrayal_israel(sentence_list)\n",
    "            # dict_newspaper[\"portrayal_israeli\"] = portrayal_israeli(sentence_list)\n",
    "            # dict_newspaper[\"portrayal_idf\"] = portrayal_idf(sentence_list)\n",
    "            # dict_newspaper[\"portrayal_netanyahu\"] = portrayal_netanyahu(sentence_list)\n",
    "            dict_newspaper[\"articles\"] = article_list\n",
    "            dict_newspaper[\"preprocessed\"] = sentence_list\n",
    "\n",
    "            # actually fill out the values for model-related keys\n",
    "            dict_newspaper[\"model_location\"], dict_newspaper[\"vectors_txt_location\"], dict_newspaper[\"vectors_bin_location\"] = train(newspaper, sentence_list)\n",
    "\n",
    "            # Load the model from a file\n",
    "            model = Word2Vec.load(f\"{newspaper/newspaper}_w2v.model\")\n",
    "\n",
    "            palestinian_words = [\"palestine\", \"palestinian\", \"hamas\", \"sinwar\"]\n",
    "            israeli_words = [\"israel\", \"israeli\", \"idf\", \"netanyahu\"]\n",
    "\n",
    "            # positive categories: general (good etc), victim, \n",
    "            positive_portrayal_words = [\"positive\", \"good\", \"victim\", \"resilient\", \"justified\", \"defenders\", \"innocent\", \"rightful\",\"humane\"]\n",
    "            negative_portrayal_words = [\"positive\", \"good\", \"victim\", \"resilient\", \"justified\", \"defenders\", \"innocent\", \"rightful\"]\n",
    "\n",
    "            calculate_portrayal(model,\n",
    "                                palestinian_words\n",
    "                                , positive_portrayal_words, negative_portrayal_words)\n",
    "\n",
    "\n",
    "# ### Potential Portrayal Words\n",
    "# Positive: positive, good, victim, humane, heroic, brave, noble, resilient, justified, courageous, victorious, liberating, righteous, defenders, innocent\n",
    "# Negative: negative, bad, aggressor, attacker, aggressive, brutal, oppressive, merciless, barbaric, ruthless, massacra\n",
    "# invaders, terrorist\n",
    "# terroristic, dictatorial, destructive, illegal, corrupt, authoritarian, regressive, settler\n",
    "#\n",
    "# Find word frequency for these words"
   ],
   "id": "aa6c3f9e44a3a942",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Save and load preprocessed newspapers",
   "id": "cea70a66012054ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def save_newspaper_dict(newspaper_dict):\n",
    "    # File path for the JSON file\n",
    "    file_path = \"preprocessed_newspaper_articles.json\"\n",
    "\n",
    "    # Step 1: Load existing data if the file exists, otherwise start with an empty list\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as json_file:\n",
    "            data = json.load(json_file)  # Load existing data\n",
    "        for key,value in newspaper_dict:\n",
    "            if key not in data:\n",
    "                data[\"key\"] = value\n",
    "\n",
    "    else:\n",
    "        data = newspaper_dict\n",
    "\n",
    "    # Step 3: Write the updated data back to the file\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n"
   ],
   "id": "5c82d0b37ad9001e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "save_newspaper_dict(newspaper_dict)",
   "id": "cc150acbd50f871c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "with open(\"preprocessed_newspaper_articles.json\", \"r\") as json_file:\n",
    "    loaded_newspaper_dict = json.load(json_file)\n",
    "    print(loaded_newspaper_dict)"
   ],
   "id": "75cfbdd9c26d6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Find corpus size for cnn",
   "id": "198f79c541342fe8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a7527a0b36eac216",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train word2vec on cnn.com",
   "id": "1308917cf59f87fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Prepare sentences for Word2Vec\n",
    "sentences = loaded_newspaper_dict[\"cnn.com\"] # Each newspaper's corpus is one \"document\"\n",
    "print(sentences)\n",
    "# Train Word2Vec model\n",
    "# Initialize the model with parameters\n",
    "model = Word2Vec(sentences=sentences, vector_size=300, window=5, min_count=10, sg=1, workers=4, negative=20)\n",
    "\n",
    "# Train the model\n",
    "model.train(sentences, total_examples=len(sentences), epochs=20)"
   ],
   "id": "fc38a9adcb59c045",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# model.save(\"cnn_w2v.model\")\n",
    "# Save just the word vectors in a text format\n",
    "model.wv.save_word2vec_format(\"cnn_w2v_vectors.txt\", binary=False)\n",
    "\n",
    "# To save in binary format:\n",
    "model.wv.save_word2vec_format(\"cnn_w2v_vectors.bin\", binary=True)\n"
   ],
   "id": "bca1cb1d25189a35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load the model",
   "id": "cd50ddc78675e441"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the model from a file\n",
    "model = Word2Vec.load(\"cnn_w2v.model\")\n",
    "\n",
    "# Now you can use the model\n",
    "print(model.wv.most_similar(\"israeli\"))  # Replace \"your_word\" with the word you're interested in\n",
    "\n",
    "# Load the word vectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format(\"cnn_W2v_vectors.txt\", binary=False)"
   ],
   "id": "4f2e49512f1caad2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the vector for a word\n",
    "vector = model.wv[\"idf\"]\n",
    "\n",
    "# Find most similar words\n",
    "similar_words = model.wv.most_similar(\"bad\")\n",
    "print(similar_words)\n",
    "\n",
    "# Calculate similarity\n",
    "similarity = model.wv.similarity(\"palestine\", \"victim\")\n",
    "print(f\"Similarity between 'palestine' and 'victim': {similarity}\")\n",
    "\n",
    "# Calculate similarity\n",
    "similarity = model.wv.similarity(\"israel\", \"victim\")\n",
    "print(f\"Similarity between 'israel' and 'victim': {similarity}\")"
   ],
   "id": "a07a9ff1cc647a56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Potential Portrayal Words\n",
    "Positive: positive, good, victim, humane, heroic, brave, noble, resilient, justified, courageous, victorious, liberating, righteous, defenders, innocent\n",
    "Negative: negative, bad, aggressor, attacker, aggressive, brutal, oppressive, merciless, barbaric, ruthless, massacra\n",
    "invaders, terrorist\n",
    "terroristic, dictatorial, destructive, illegal, corrupt, authoritarian, regressive, settler\n",
    "\n",
    "Find word frequency for these words"
   ],
   "id": "2a8fba3574ed1af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# MASTER FUNCTION"
   ],
   "id": "a7d01fda58d2b0e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "with open(\"data/news-data-extracted.json\", \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "newspaper = data[\"cnn.com\"] # newspaper will be a dictionary of articles with values being url, date, authors, text etc.\n",
    "newspaper_articles = []\n",
    "\n",
    "for article in newspaper:\n",
    "    newspaper_articles.append(article[\"text\"])\n",
    "\n",
    "print(newspaper_articles)"
   ],
   "id": "1ce08ef33bc2ecea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "newspaper",
   "id": "2b4d516d852bbf82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# take a scraped newspaper in, which will be a key to the dictionary we download: example: data['cnn.com']\n",
    "# get corpus size before preprocessing\n",
    "# preprocess the newspaper\n",
    "# get corpus size after preprocessing\n",
    "# find word frequency for israel, palestine, idf, hamas, gaza, west bank\n",
    "# save it to the preprocessed_newspaper_articles dictionary\n",
    "# train word2vec on it\n",
    "# measure portrayal for both sides\n",
    "# all this metadata & results in a dict, and preprocessed corpus to preprocessed_newspaper_articles\n",
    "def master(extracted_file, preprocessed_file, newspaper_name):\n",
    "\n",
    "    import json\n",
    "    with open(extracted_file, \"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    newspaper = data[f\"{newspaper_name}\"] # newspaper will be a list of dictionaries, each dictionary representing an article with keys being url, date, authors, text etc.\n",
    "    newspaper_articles = newspaper['text'] # newspaper_articles will be\n",
    "\n",
    "\n",
    "    pass"
   ],
   "id": "5006b0536f5c4d7a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
