{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eaba59c3bf0d827",
   "metadata": {},
   "source": [
    "# Newspaper portrayal analysis of Israel and Palestine\n",
    "In this project, we train a word embedding model (a model that can assign meaningful vectors to words), specifically Word2Vec, on multiples newspapers' corpora. We create these corpora by scraping websites of different sources. If you would like to see how we scraped, please check out the github repository for the project: https://github.com/McGill-AI-Lab/news-bias-model\n",
    "\n",
    "#### Note:\n",
    "In the following jupyter notebook, we had to delete some of the cell outputs due to either them being too large, or some copyright constraints. Please know that we ran all of the code in the notebook.\n",
    "\n",
    "### Abstract\n",
    "lorem ipsum\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de74c7d7aa56c9d",
   "metadata": {},
   "source": [
    "\n",
    "### Access and preprocess our data"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We access our data in 'data/news-data-extracted.json', which you should also have access to through the repository. This file is a dictionary, with keys corresponding to different newspapers, and for each newspaper key, the corresponding value is a list of dictionaries, each dictionary containing key-value pairs for a single article. Keys include: url, title, authors, date, text",
   "id": "200de46c6bcb6b2b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T13:56:59.614870Z",
     "start_time": "2024-11-29T13:56:59.381713Z"
    }
   },
   "source": [
    "import json\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open('data/news-data-extracted.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Print the data\n",
    "first_article_data = data[\"cnn.com\"][0] # cnn is the key to a value which is a list of dictionaries, we get the first dictionary (article) of that list of dictionary\n",
    "first_article = first_article_data[\"text\"] # we get the article text instead of getting the whole dictionary that includes url, title, authors, etc. \n",
    "print(first_article_data)"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "1d65473b172ae9cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T13:59:14.025611Z",
     "start_time": "2024-11-29T13:59:14.002029Z"
    }
   },
   "source": "first_article[0] # since we get a string, this should return the first letter in the article",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We need to preprocess our data. Preprocessing includes dividing articles into sentences using nltk library, since Word2Vec is trained by using list of words (sentences). Nltk uses a machine learning model to decide how to divide an article into sentences, so there will be some inaccuracies, however we can ignore these. After, we want all words to be lowercase. We want to remove all extremely high-frequency words which don't really contribute to any of the word embeddings for other words they co-occur with as these high-frequency word co-occur with a big portion of our corpus. These words are called \"stop words\" and some example would be \"I\", \"you\", \"of\", \"there\" etc. Then, we lemmatize words, i.e. try to convert each word to their root (running -> run). The goal of this process is that so we have more information about the word \"run\", instead of the information being distributed between various forms of the word (\"runs\", \"running\", \"ran\"). For more information on lemmatizers: https://www.geeksforgeeks.org/python-lemmatization-with-nltk/. Finally, we remove punctuation and put all of these functions in one \"preprocess\" function.",
   "id": "c73650353e8aac36"
  },
  {
   "cell_type": "code",
   "id": "309ac0a633e6010e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T14:09:41.691839Z",
     "start_time": "2024-11-29T14:09:37.007489Z"
    }
   },
   "source": [
    "from gensim.utils import tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# tokenizes article into sentences, which are also tokenized into words\n",
    "def tokenize_article(article):\n",
    "    tokenized_article = []\n",
    "    sentences  = sent_tokenize(article, language=\"english\") # divide article into sentences\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokenized_sentence = tokenize(sentence) # divide sentences into words\n",
    "        tokenized_article.append(tokenized_sentence) \n",
    "    return tokenized_article\n",
    "\n",
    "# makes each word lowercase\n",
    "def lowercase(tokenized_article):\n",
    "    lowercase_article = []\n",
    "\n",
    "    for sentence in tokenized_article:\n",
    "        current_sentence = []\n",
    "        for word in sentence:\n",
    "            current_sentence.append(word.lower())\n",
    "        lowercase_article.append(current_sentence)\n",
    "\n",
    "    return lowercase_article\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(tokenized_article):\n",
    "    # Iterate over the index and content of each sentence\n",
    "    for i in range(len(tokenized_article)):\n",
    "        # Create a new list for the filtered sentence\n",
    "        filtered_sentence = []\n",
    "        for word in tokenized_article[i]:\n",
    "            if word not in stop_words:\n",
    "                filtered_sentence.append(word)\n",
    "        # Replace the original sentence with the filtered sentence\n",
    "        tokenized_article[i] = filtered_sentence\n",
    "    return tokenized_article\n",
    "\n",
    "def lammetization(tokenized_article):\n",
    "    lammetizer = WordNetLemmatizer()\n",
    "\n",
    "    lammetized_article = []\n",
    "\n",
    "    for sentence in tokenized_article:\n",
    "        current_sentence = []\n",
    "        for word in sentence:\n",
    "            current_sentence.append(lammetizer.lemmatize(word))\n",
    "        lammetized_article.append(current_sentence)\n",
    "\n",
    "    return lammetized_article\n",
    "\n",
    "\n",
    "def remove_punctuation(tokenized_article):\n",
    "    punc_removed_article = []\n",
    "\n",
    "    for sentence in tokenized_article:\n",
    "        punc_removed_sentence = []\n",
    "        for word in sentence:\n",
    "            # Split by punctuation, filter out empty strings, and join back if needed\n",
    "            split_word = ''.join(re.split(r\"[^\\w]+\", word))\n",
    "            if split_word:  # Add non-empty words only\n",
    "                punc_removed_sentence.append(split_word)\n",
    "\n",
    "        punc_removed_article.append(punc_removed_sentence)\n",
    "\n",
    "    return punc_removed_article\n",
    "\n",
    "def preprocess_article(article):\n",
    "    t_article = tokenize_article(article)\n",
    "    l_article = lowercase(t_article)\n",
    "    r_article = remove_stopwords(l_article)\n",
    "    la_article = lammetization(r_article)\n",
    "    re_article = remove_punctuation(la_article)\n",
    "    return re_article"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "6278a1090040ea31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T14:09:57.221265Z",
     "start_time": "2024-11-29T14:09:57.214157Z"
    }
   },
   "source": "print(stop_words)",
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "e6a5c2a3d799cc88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T14:10:26.903194Z",
     "start_time": "2024-11-29T14:10:26.892867Z"
    }
   },
   "source": [
    "preprocessed = preprocess_article(first_article)\n",
    "print(preprocessed) # this is how a preprocessed article looks like"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953133fe35df9b92",
   "metadata": {},
   "source": [
    "def create_article_list(extracted_file, newspaper_name): \n",
    "    import json\n",
    "    with open(extracted_file, \"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    newspaper = data[f\"{newspaper_name}\"] # newspaper will be a dictionary of articles with values being url, date, authors, text etc.\n",
    "    newspaper_articles = []\n",
    "    \n",
    "    for article in newspaper:\n",
    "        newspaper_articles.append(article[\"text\"])\n",
    "    \n",
    "    print(newspaper_articles)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94847e14cc9be247",
   "metadata": {},
   "source": [
    "def preprocess_newspaper(newspaper, newspaper_name, newspaper_dict):\n",
    "    newspaper_dict[f\"{newspaper_name}\"] = []\n",
    "    i = 0\n",
    "    for article_data in newspaper:\n",
    "        text = article_data['text']\n",
    "        newspaper_dict[f\"{newspaper_name}\"].extend(preprocess_article(text))  # extends preproccessed\n",
    "        # articles to newspaper's article list\n",
    "        print(f\"{newspaper_name}: article {i} preprocessed\")\n",
    "        i += 1\n",
    "    return newspaper_dict"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53184256486a4c43",
   "metadata": {},
   "source": [
    "# Lets try with CNN\n",
    "newspaper = data[\"cnn.com\"]\n",
    "newspaper_dict = {}\n",
    "newspaper_dict = preprocess_newspaper(newspaper, \"cnn.com\", newspaper_dict)\n",
    "print(newspaper_dict)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cea70a66012054ff",
   "metadata": {},
   "source": [
    "### Save and load preprocessed newspapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c82d0b37ad9001e",
   "metadata": {},
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def save_newspaper_dict(newspaper_dict):\n",
    "    # File path for the JSON file\n",
    "    file_path = \"preprocessed_newspaper_articles.json\"\n",
    "\n",
    "    # Step 1: Load existing data if the file exists, otherwise start with an empty list\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as json_file:\n",
    "            data = json.load(json_file)  # Load existing data\n",
    "        for key,value in newspaper_dict:\n",
    "            if key not in data:\n",
    "                data[\"key\"] = value\n",
    "\n",
    "    else:\n",
    "        data = newspaper_dict\n",
    "\n",
    "    # Step 3: Write the updated data back to the file\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc150acbd50f871c",
   "metadata": {},
   "source": [
    "save_newspaper_dict(newspaper_dict)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cfbdd9c26d6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T22:18:29.528613Z",
     "start_time": "2024-11-18T22:18:28.394604Z"
    }
   },
   "source": [
    "import json\n",
    "with open(\"preprocessed_newspaper_articles.json\", \"r\") as json_file:\n",
    "    loaded_newspaper_dict = json.load(json_file)\n",
    "    print(loaded_newspaper_dict)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "198f79c541342fe8",
   "metadata": {},
   "source": [
    "Find corpus size for cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7527a0b36eac216",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T22:23:55.842421Z",
     "start_time": "2024-11-18T22:23:55.729008Z"
    }
   },
   "source": [
    "def corpus_size(dict, newspaper):\n",
    "    corpus = dict[f\"{newspaper}\"]\n",
    "    \n",
    "    corpus_size = 0\n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            corpus_size += 1\n",
    "    \n",
    "    return corpus_size\n",
    "\n",
    "cs = corpus_size(loaded_newspaper_dict, \"cnn.com\")\n",
    "print(cs)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1308917cf59f87fd",
   "metadata": {},
   "source": [
    "### Train word2vec on cnn.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc38a9adcb59c045",
   "metadata": {},
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Prepare sentences for Word2Vec\n",
    "sentences = loaded_newspaper_dict[\"cnn.com\"] # Each newspaper's corpus is one \"document\"\n",
    "print(sentences)\n",
    "# Train Word2Vec model\n",
    "# Initialize the model with parameters\n",
    "model = Word2Vec(sentences=sentences, vector_size=300, window=5, min_count=10, sg=1, workers=4, negative=20)\n",
    "\n",
    "# Train the model\n",
    "model.train(sentences, total_examples=len(sentences), epochs=20)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca1cb1d25189a35",
   "metadata": {},
   "source": [
    "# model.save(\"cnn_w2v.model\")\n",
    "# Save just the word vectors in a text format\n",
    "model.wv.save_word2vec_format(\"cnn_w2v_vectors.txt\", binary=False)\n",
    "\n",
    "# To save in binary format:\n",
    "model.wv.save_word2vec_format(\"cnn_w2v_vectors.bin\", binary=True)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cd50ddc78675e441",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2e49512f1caad2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T21:32:13.523442Z",
     "start_time": "2024-11-18T21:32:04.566463Z"
    }
   },
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the model from a file\n",
    "model = Word2Vec.load(\"cnn_w2v.model\")\n",
    "\n",
    "# Now you can use the model\n",
    "print(model.wv.most_similar(\"israeli\"))  # Replace \"your_word\" with the word you're interested in\n",
    "\n",
    "# Load the word vectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format(\"cnn_W2v_vectors.txt\", binary=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07a9ff1cc647a56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T22:38:32.738420Z",
     "start_time": "2024-11-18T22:38:32.728437Z"
    }
   },
   "source": [
    "# Get the vector for a word\n",
    "vector = model.wv[\"idf\"]\n",
    "\n",
    "# Find most similar words\n",
    "similar_words = model.wv.most_similar(\"bad\")\n",
    "print(similar_words)\n",
    "\n",
    "# Calculate similarity\n",
    "similarity = model.wv.similarity(\"palestine\", \"victim\")\n",
    "print(f\"Similarity between 'palestine' and 'victim': {similarity}\")\n",
    "\n",
    "# Calculate similarity\n",
    "similarity = model.wv.similarity(\"israel\", \"victim\")\n",
    "print(f\"Similarity between 'israel' and 'victim': {similarity}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2a8fba3574ed1af",
   "metadata": {},
   "source": [
    "### Potential Portrayal Words\n",
    "Positive: positive, good, victim, humane, heroic, brave, noble, resilient, justified, courageous, victorious, liberating, righteous, defenders\n",
    "Negative: negative, bad, aggressor, attacker, aggressive, brutal, oppressive, merciless, barbaric, ruthless, massacra\n",
    "invaders, terrorist\n",
    "terroristic, dictatorial, destructive, illegal, corrupt, authoritarian, regressive, settler\n",
    "\n",
    "Find word frequency for these words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d01fda58d2b0e2",
   "metadata": {},
   "source": [
    "\n",
    "# MASTER FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce08ef33bc2ecea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T22:23:01.244548Z",
     "start_time": "2024-11-24T22:23:00.696592Z"
    }
   },
   "source": [
    "import json\n",
    "with open(\"data/news-data-extracted.json\", \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "newspaper = data[\"cnn.com\"] # newspaper will be a dictionary of articles with values being url, date, authors, text etc.\n",
    "newspaper_articles = []\n",
    "\n",
    "for article in newspaper:\n",
    "    newspaper_articles.append(article[\"text\"])\n",
    "\n",
    "print(newspaper_articles)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4d516d852bbf82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T22:21:58.624609Z",
     "start_time": "2024-11-24T22:21:58.478317Z"
    }
   },
   "source": [
    "newspaper"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5006b0536f5c4d7a",
   "metadata": {},
   "source": [
    "# take a scraped newspaper in, which will be a key to the dictionary we download: example: data['cnn.com']\n",
    "# get corpus size before preprocessing\n",
    "# preprocess the newspaper\n",
    "# get corpus size after preprocessing\n",
    "# find word frequency for israel, palestine, idf, hamas, gaza, west bank\n",
    "# save it to the preprocessed_newspaper_articles dictionary\n",
    "# train word2vec on it\n",
    "# measure portrayal for both sides\n",
    "# all this metadata & results in a dict, and preprocessed corpus to preprocessed_newspaper_articles\n",
    "def master(extracted_file, preprocessed_file, newspaper_name):\n",
    "    \n",
    "    import json\n",
    "    with open(extracted_file, \"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "        \n",
    "    newspaper = data[f\"{newspaper_name}\"] # newspaper will be a list of dictionaries, each dictionary representing an article with keys being url, date, authors, text etc.\n",
    "    newspaper_articles = newspaper['text'] # newspaper_articles will be \n",
    "    \n",
    "    \n",
    "    pass"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
