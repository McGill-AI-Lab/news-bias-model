{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eaba59c3bf0d827",
   "metadata": {},
   "source": [
    "# Newspaper portrayal analysis of Israel and Palestine\n",
    "In this project, we train a word embedding model (a model that can assign meaningful vectors to words), specifically Word2Vec, on multiples newspapers' corpora. We create these corpora by scraping websites of different sources. If you would like to see how we scraped, please check out the github repository for the project: https://github.com/McGill-AI-Lab/news-bias-model\n",
    "\n",
    "#### Note:\n",
    "In the following jupyter notebook, we had to delete some of the cell outputs due to either them being too large, or some copyright constraints. Please know that we ran all of the code in the notebook.\n",
    "\n",
    "### Abstract\n",
    "lorem ipsum\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de74c7d7aa56c9d",
   "metadata": {},
   "source": [
    "\n",
    "### Access and preprocess our data"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We access our data in 'data/news-data-extracted.json', which you should also have access to through the repository. This file is a dictionary, with keys corresponding to different newspapers, and for each newspaper key, the corresponding value is a list of dictionaries, each dictionary containing key-value pairs for a single article. Keys include: url, title, authors, date, text",
   "id": "200de46c6bcb6b2b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T13:56:59.614870Z",
     "start_time": "2024-11-29T13:56:59.381713Z"
    }
   },
   "source": [
    "import json\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open('data/news-data-extracted.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Print the data\n",
    "first_article_data = data[\"cnn.com\"][0] # cnn is the key to a value which is a list of dictionaries, we get the first dictionary (article) of that list of dictionary\n",
    "first_article = first_article_data[\"text\"] # we get the article text instead of getting the whole dictionary that includes url, title, authors, etc. \n",
    "print(first_article_data)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://www.cnn.com/2020/01/23/opinions/auschwitz-anniversary-anti-semitism-fears-linger-andelman/index.html', 'title': 'World leaders in Jerusalem show battle against anti-Semitism not yet a victory (opinion)', 'authors': ['David A. Andelman'], 'date': '2020-01-23 00:00:00', 'text': 'Editors Note: David A. Andelman, Executive Director of The RedLines Project, is a contributor to CNN where his columns won the Deadline Club Award for Best Opinion Writing. Author of A Shattered Peace: Versailles 1919 and the Price We Pay Today, and the forthcoming A Red Line in the Sand: Diplomacy, Strategy and a History of Wars That Almost Happened, he was formerly a foreign correspondent for The New York Times and CBS News in Europe and Asia. Follow him on Twitter @DavidAndelman. The views expressed in this commentary are his own. View more opinion on CNN.CNN The world converged on Jerusalem this week to observe the 75th anniversary of the liberation of the Auschwitz death camp  and with a collective determination to battle anti-Semitism in its many forms. At the same time, the gathering exposed a number of old and festering political wounds, which threatened to weaken the impact of it.Some 40 heads of state or government from Russian President Vladimir Putin to French President Emmanuel Macron traveled to Israels Yad Vashem Holocaust memorial to pay homage to those who died and to pledge never again. The actual anniversary of the liberation of Auschwitz, where at least 1.1 million were slaughtered in the Nazi death camp, will be marked on Monday in Poland.In Israel, though, there were noticeable absentees and a succession of contretemps.Poland and Lithuania, for example, have accused Putin of reviving Stalinist-era tropes when hes claimed the Soviet Union saved the world from Nazism and framed the Polish people as participating in the Holocaust. So, when Putin was given a speaking role at Yad Vashem, Presidents Andrzej Duda of Poland and Lithuanias Gitanas Nauseda declined to attend.Meanwhile, French President Emmanuel Macron got into a minor shouting match with Israeli police, who sought to enter the medieval Church of Saint Anne in Jerusalem  French territory since the 19th century  ahead of the French leader. (Notably, a nearly identical incident happened with the late French President Jacques Chirac in 1996).Later, however, during a group photo of the world leaders at the home of Israeli President Reuven Rivlin, Macron was strategically pictured front and center between hosts Rivlin and Prime Minister Benjamin Netanyahu.In a fight for his own political life, under indictment for corruption and facing new elections in March, Netanyahu has been making the most of this weeks events. He even leaned into his role as statesman, strolling for television cameras with Putin and then taking him on a visit to the mother of Naama Issachar, an Israeli-American woman imprisoned in Russia.This awkward moment as leader of a country keeping a womans child in prison was just one of the issues facing Putin at the ceremonies. He must decide whether to free Issachar, who is serving seven and a half years in prison after marijuana was found in her luggage while she transited a Moscow airport. (Issachar maintains the drugs were planted on her, and she was forced into signing a false confession.)Israeli media suggested Putin was demanding a number of gestures from Israel before Putin agrees to release her, a decision which seems increasingly likely after the high-octane campaign on her behalf.And, in Paris, Macron is facing demands, reiterated by Netanyahu during their meeting, to deal with the murder of Sarah Halimi, a 65-year-old French Jew who was thrown from her Paris apartment window in 2017. A French court ruled that the killer was not criminally responsible. Despite the court ruling, many protesters in France believe the killing of Halimi to be an anti-Semitic act.Meanwhile, Macron  as well as leaders across the globe  have a broader range of issues to address regarding growing anti-Semitism. The Conference on Jewish Material Claims Against Germany, a nearly 60-year-old organization dealing with Holocaust education, released a survey this week showing that 57% of all French adults and 69% of millennials and Generation Z did not know 6 million Jews were killed in the Holocaust.Get our weekly newsletter Sign up for CNN Opinions new newsletter. Join us on Twitter and FacebookMany members of the Jewish community question whether world leaders were prepared to demonstrate, as King Felipe VI of Spain said, an unyielding commitment to fighting the ignorant intolerance, hatred and the total lack of human empathy that permitted and gave birth to the Holocaust.The skepticism remains because despite wide-ranging efforts by well-meaning world leaders, anti-Semitic incidents and attacks on Jews and Jewish establishments are on the rise across Europe and the United StatesDefining and then dealing with anti-Semitism remain two critical hurdles that even the best-intentioned officials seem ill-equipped to handle. But, as King Felipe concluded, There is no room for indifference in the presence of racism, xenophobia, hate speech and anti-Semitism.'}\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "1d65473b172ae9cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T13:59:14.025611Z",
     "start_time": "2024-11-29T13:59:14.002029Z"
    }
   },
   "source": "first_article[0] # since we get a string, this should return the first letter in the article",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We need to preprocess our data. Preprocessing includes dividing articles into sentences using nltk library, since Word2Vec is trained by using list of words (sentences). Nltk uses a machine learning model to decide how to divide an article into sentences, so there will be some inaccuracies, however we can ignore these. After, we want all words to be lowercase. We want to remove all extremely high-frequency words which don't really contribute to any of the word embeddings for other words they co-occur with as these high-frequency word co-occur with a big portion of our corpus. These words are called \"stop words\" and some example would be \"I\", \"you\", \"of\", \"there\" etc. Then, we lemmatize words, i.e. try to convert each word to their root (running -> run). The goal of this process is that so we have more information about the word \"run\", instead of the information being distributed between various forms of the word (\"runs\", \"running\", \"ran\"). For more information on lemmatizers: https://www.geeksforgeeks.org/python-lemmatization-with-nltk/. Finally, we remove punctuation and put all of these functions in one \"preprocess\" function.",
   "id": "c73650353e8aac36"
  },
  {
   "cell_type": "code",
   "id": "309ac0a633e6010e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T14:09:41.691839Z",
     "start_time": "2024-11-29T14:09:37.007489Z"
    }
   },
   "source": [
    "from gensim.utils import tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# tokenizes article into sentences, which are also tokenized into words\n",
    "def tokenize_article(article):\n",
    "    tokenized_article = []\n",
    "    sentences  = sent_tokenize(article, language=\"english\") # divide article into sentences\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokenized_sentence = tokenize(sentence) # divide sentences into words\n",
    "        tokenized_article.append(tokenized_sentence) \n",
    "    return tokenized_article\n",
    "\n",
    "# makes each word lowercase\n",
    "def lowercase(tokenized_article):\n",
    "    lowercase_article = []\n",
    "\n",
    "    for sentence in tokenized_article:\n",
    "        current_sentence = []\n",
    "        for word in sentence:\n",
    "            current_sentence.append(word.lower())\n",
    "        lowercase_article.append(current_sentence)\n",
    "\n",
    "    return lowercase_article\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(tokenized_article):\n",
    "    # Iterate over the index and content of each sentence\n",
    "    for i in range(len(tokenized_article)):\n",
    "        # Create a new list for the filtered sentence\n",
    "        filtered_sentence = []\n",
    "        for word in tokenized_article[i]:\n",
    "            if word not in stop_words:\n",
    "                filtered_sentence.append(word)\n",
    "        # Replace the original sentence with the filtered sentence\n",
    "        tokenized_article[i] = filtered_sentence\n",
    "    return tokenized_article\n",
    "\n",
    "def lammetization(tokenized_article):\n",
    "    lammetizer = WordNetLemmatizer()\n",
    "\n",
    "    lammetized_article = []\n",
    "\n",
    "    for sentence in tokenized_article:\n",
    "        current_sentence = []\n",
    "        for word in sentence:\n",
    "            current_sentence.append(lammetizer.lemmatize(word))\n",
    "        lammetized_article.append(current_sentence)\n",
    "\n",
    "    return lammetized_article\n",
    "\n",
    "\n",
    "def remove_punctuation(tokenized_article):\n",
    "    punc_removed_article = []\n",
    "\n",
    "    for sentence in tokenized_article:\n",
    "        punc_removed_sentence = []\n",
    "        for word in sentence:\n",
    "            # Split by punctuation, filter out empty strings, and join back if needed\n",
    "            split_word = ''.join(re.split(r\"[^\\w]+\", word))\n",
    "            if split_word:  # Add non-empty words only\n",
    "                punc_removed_sentence.append(split_word)\n",
    "\n",
    "        punc_removed_article.append(punc_removed_sentence)\n",
    "\n",
    "    return punc_removed_article\n",
    "\n",
    "def preprocess_article(article):\n",
    "    t_article = tokenize_article(article)\n",
    "    l_article = lowercase(t_article)\n",
    "    r_article = remove_stopwords(l_article)\n",
    "    la_article = lammetization(r_article)\n",
    "    re_article = remove_punctuation(la_article)\n",
    "    return re_article"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "6278a1090040ea31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T14:09:57.221265Z",
     "start_time": "2024-11-29T14:09:57.214157Z"
    }
   },
   "source": "print(stop_words)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'up', 'my', 'am', 'how', 'through', 'yourself', 'so', 'you', 'yourselves', 'by', 'is', 'had', 'itself', \"shouldn't\", \"that'll\", 'has', 'about', 'each', 'him', \"she's\", 'been', 'didn', 'its', 'our', 'that', 't', 'will', 'same', 'then', 'isn', 'shan', 'any', 'which', 'out', 'down', 'of', \"you're\", 'who', 'mustn', 'me', 'at', 'the', \"won't\", 'weren', 'herself', 'haven', 'if', 'hasn', 'her', 'doesn', 'his', 'as', 'again', 'and', 'an', 'because', 'are', 'their', \"you've\", \"should've\", 'ain', 'won', \"don't\", 've', 'll', 'to', 'do', 'once', \"it's\", 'ourselves', 'just', 'whom', 're', 'further', 'only', \"isn't\", 'wouldn', 'for', 'needn', 'he', 'have', 'below', \"doesn't\", 'against', 'aren', 'here', 'they', 'both', 'a', 'm', 'or', 'mightn', \"mightn't\", 'more', \"hadn't\", 'after', 'above', \"wouldn't\", 'she', 'theirs', 'with', 'over', 'than', 'own', 'such', 'we', 'nor', \"shan't\", \"aren't\", 'himself', 'too', 'while', 'these', 'now', 'them', 'into', 'those', 'was', 'all', 'couldn', \"wasn't\", 'until', 'did', 'off', 'other', 'hadn', 'be', 'before', 'ours', 'i', \"mustn't\", 'don', 'but', \"needn't\", 'themselves', 's', 'doing', \"couldn't\", 'under', 'it', 'some', 'this', 'during', 'there', \"hasn't\", \"weren't\", 'on', 'hers', 'being', 'from', 'can', 'having', 'should', 'where', \"you'd\", 'yours', 'does', 'between', 'what', 'your', 'were', 'why', 'myself', 'ma', 'shouldn', 'y', 'not', 'when', \"you'll\", 'o', 'in', \"haven't\", 'd', \"didn't\", 'few', 'very', 'no', 'wasn', 'most'}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "e6a5c2a3d799cc88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T14:10:26.903194Z",
     "start_time": "2024-11-29T14:10:26.892867Z"
    }
   },
   "source": [
    "preprocessed = preprocess_article(first_article)\n",
    "print(preprocessed) # this is how a preprocessed article looks like"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['editor', 'note', 'david', 'andelman', 'executive', 'director', 'redlines', 'project', 'contributor', 'cnn', 'column', 'deadline', 'club', 'award', 'best', 'opinion', 'writing'], ['author', 'shattered', 'peace', 'versailles', 'price', 'pay', 'today', 'forthcoming', 'red', 'line', 'sand', 'diplomacy', 'strategy', 'history', 'war', 'almost', 'happened', 'formerly', 'foreign', 'correspondent', 'new', 'york', 'time', 'cbs', 'news', 'europe', 'asia'], ['follow', 'twitter', 'davidandelman'], ['view', 'expressed', 'commentary'], ['view', 'opinion', 'cnn', 'cnn', 'world', 'converged', 'jerusalem', 'week', 'observe', 'th', 'anniversary', 'liberation', 'auschwitz', 'death', 'camp', 'collective', 'determination', 'battle', 'anti', 'semitism', 'many', 'form'], ['time', 'gathering', 'exposed', 'number', 'old', 'festering', 'political', 'wound', 'threatened', 'weaken', 'impact', 'head', 'state', 'government', 'russian', 'president', 'vladimir', 'putin', 'french', 'president', 'emmanuel', 'macron', 'traveled', 'israel', 'yad', 'vashem', 'holocaust', 'memorial', 'pay', 'homage', 'died', 'pledge', 'never'], ['actual', 'anniversary', 'liberation', 'auschwitz', 'least', 'million', 'slaughtered', 'nazi', 'death', 'camp', 'marked', 'monday', 'poland', 'israel', 'though', 'noticeable', 'absentee', 'succession', 'contretemps', 'poland', 'lithuania', 'example', 'accused', 'putin', 'reviving', 'stalinist', 'era', 'trope', 'he', 'claimed', 'soviet', 'union', 'saved', 'world', 'nazism', 'framed', 'polish', 'people', 'participating', 'holocaust'], ['putin', 'given', 'speaking', 'role', 'yad', 'vashem', 'president', 'andrzej', 'duda', 'poland', 'lithuania', 'gitana', 'nauseda', 'declined', 'attend', 'meanwhile', 'french', 'president', 'emmanuel', 'macron', 'got', 'minor', 'shouting', 'match', 'israeli', 'police', 'sought', 'enter', 'medieval', 'church', 'saint', 'anne', 'jerusalem', 'french', 'territory', 'since', 'th', 'century', 'ahead', 'french', 'leader'], ['notably', 'nearly', 'identical', 'incident', 'happened', 'late', 'french', 'president', 'jacques', 'chirac', 'later', 'however', 'group', 'photo', 'world', 'leader', 'home', 'israeli', 'president', 'reuven', 'rivlin', 'macron', 'strategically', 'pictured', 'front', 'center', 'host', 'rivlin', 'prime', 'minister', 'benjamin', 'netanyahu', 'fight', 'political', 'life', 'indictment', 'corruption', 'facing', 'new', 'election', 'march', 'netanyahu', 'making', 'week', 'event'], ['even', 'leaned', 'role', 'statesman', 'strolling', 'television', 'camera', 'putin', 'taking', 'visit', 'mother', 'naama', 'issachar', 'israeli', 'american', 'woman', 'imprisoned', 'russia', 'awkward', 'moment', 'leader', 'country', 'keeping', 'woman', 'child', 'prison', 'one', 'issue', 'facing', 'putin', 'ceremony'], ['must', 'decide', 'whether', 'free', 'issachar', 'serving', 'seven', 'half', 'year', 'prison', 'marijuana', 'found', 'luggage', 'transited', 'moscow', 'airport'], ['issachar', 'maintains', 'drug', 'planted', 'forced', 'signing', 'false', 'confession'], ['israeli', 'medium', 'suggested', 'putin', 'demanding', 'number', 'gesture', 'israel', 'putin', 'agrees', 'release', 'decision', 'seems', 'increasingly', 'likely', 'high', 'octane', 'campaign', 'behalf', 'paris', 'macron', 'facing', 'demand', 'reiterated', 'netanyahu', 'meeting', 'deal', 'murder', 'sarah', 'halimi', 'year', 'old', 'french', 'jew', 'thrown', 'paris', 'apartment', 'window'], ['french', 'court', 'ruled', 'killer', 'criminally', 'responsible'], ['despite', 'court', 'ruling', 'many', 'protester', 'france', 'believe', 'killing', 'halimi', 'anti', 'semitic', 'act', 'meanwhile', 'macron', 'well', 'leader', 'across', 'globe', 'broader', 'range', 'issue', 'address', 'regarding', 'growing', 'anti', 'semitism'], ['conference', 'jewish', 'material', 'claim', 'germany', 'nearly', 'year', 'old', 'organization', 'dealing', 'holocaust', 'education', 'released', 'survey', 'week', 'showing', 'french', 'adult', 'millennials', 'generation', 'z', 'know', 'million', 'jew', 'killed', 'holocaust', 'get', 'weekly', 'newsletter', 'sign', 'cnn', 'opinion', 'new', 'newsletter'], ['join', 'u', 'twitter', 'facebookmany', 'member', 'jewish', 'community', 'question', 'whether', 'world', 'leader', 'prepared', 'demonstrate', 'king', 'felipe', 'vi', 'spain', 'said', 'unyielding', 'commitment', 'fighting', 'ignorant', 'intolerance', 'hatred', 'total', 'lack', 'human', 'empathy', 'permitted', 'gave', 'birth', 'holocaust', 'skepticism', 'remains', 'despite', 'wide', 'ranging', 'effort', 'well', 'meaning', 'world', 'leader', 'anti', 'semitic', 'incident', 'attack', 'jew', 'jewish', 'establishment', 'rise', 'across', 'europe', 'united', 'statesdefining', 'dealing', 'anti', 'semitism', 'remain', 'two', 'critical', 'hurdle', 'even', 'best', 'intentioned', 'official', 'seem', 'ill', 'equipped', 'handle'], ['king', 'felipe', 'concluded', 'room', 'indifference', 'presence', 'racism', 'xenophobia', 'hate', 'speech', 'anti', 'semitism']]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953133fe35df9b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_article_list(extracted_file, newspaper_name): \n",
    "    import json\n",
    "    with open(extracted_file, \"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    newspaper = data[f\"{newspaper_name}\"] # newspaper will be a dictionary of articles with values being url, date, authors, text etc.\n",
    "    newspaper_articles = []\n",
    "    \n",
    "    for article in newspaper:\n",
    "        newspaper_articles.append(article[\"text\"])\n",
    "    \n",
    "    print(newspaper_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94847e14cc9be247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_newspaper(newspaper, newspaper_name, newspaper_dict):\n",
    "    newspaper_dict[f\"{newspaper_name}\"] = []\n",
    "    i = 0\n",
    "    for article_data in newspaper:\n",
    "        text = article_data['text']\n",
    "        newspaper_dict[f\"{newspaper_name}\"].extend(preprocess_article(text))  # extends preproccessed\n",
    "        # articles to newspaper's article list\n",
    "        print(f\"{newspaper_name}: article {i} preprocessed\")\n",
    "        i += 1\n",
    "    return newspaper_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53184256486a4c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try with CNN\n",
    "newspaper = data[\"cnn.com\"]\n",
    "newspaper_dict = {}\n",
    "newspaper_dict = preprocess_newspaper(newspaper, \"cnn.com\", newspaper_dict)\n",
    "print(newspaper_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea70a66012054ff",
   "metadata": {},
   "source": [
    "### Save and load preprocessed newspapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c82d0b37ad9001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def save_newspaper_dict(newspaper_dict):\n",
    "    # File path for the JSON file\n",
    "    file_path = \"preprocessed_newspaper_articles.json\"\n",
    "\n",
    "    # Step 1: Load existing data if the file exists, otherwise start with an empty list\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as json_file:\n",
    "            data = json.load(json_file)  # Load existing data\n",
    "        for key,value in newspaper_dict:\n",
    "            if key not in data:\n",
    "                data[\"key\"] = value\n",
    "\n",
    "    else:\n",
    "        data = newspaper_dict\n",
    "\n",
    "    # Step 3: Write the updated data back to the file\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc150acbd50f871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_newspaper_dict(newspaper_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cfbdd9c26d6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T22:18:29.528613Z",
     "start_time": "2024-11-18T22:18:28.394604Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"preprocessed_newspaper_articles.json\", \"r\") as json_file:\n",
    "    loaded_newspaper_dict = json.load(json_file)\n",
    "    print(loaded_newspaper_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198f79c541342fe8",
   "metadata": {},
   "source": [
    "Find corpus size for cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7527a0b36eac216",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T22:23:55.842421Z",
     "start_time": "2024-11-18T22:23:55.729008Z"
    }
   },
   "outputs": [],
   "source": [
    "def corpus_size(dict, newspaper):\n",
    "    corpus = dict[f\"{newspaper}\"]\n",
    "    \n",
    "    corpus_size = 0\n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            corpus_size += 1\n",
    "    \n",
    "    return corpus_size\n",
    "\n",
    "cs = corpus_size(loaded_newspaper_dict, \"cnn.com\")\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1308917cf59f87fd",
   "metadata": {},
   "source": [
    "### Train word2vec on cnn.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc38a9adcb59c045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Prepare sentences for Word2Vec\n",
    "sentences = loaded_newspaper_dict[\"cnn.com\"] # Each newspaper's corpus is one \"document\"\n",
    "print(sentences)\n",
    "# Train Word2Vec model\n",
    "# Initialize the model with parameters\n",
    "model = Word2Vec(sentences=sentences, vector_size=300, window=5, min_count=10, sg=1, workers=4, negative=20)\n",
    "\n",
    "# Train the model\n",
    "model.train(sentences, total_examples=len(sentences), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca1cb1d25189a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"cnn_w2v.model\")\n",
    "# Save just the word vectors in a text format\n",
    "model.wv.save_word2vec_format(\"cnn_w2v_vectors.txt\", binary=False)\n",
    "\n",
    "# To save in binary format:\n",
    "model.wv.save_word2vec_format(\"cnn_w2v_vectors.bin\", binary=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd50ddc78675e441",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2e49512f1caad2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T21:32:13.523442Z",
     "start_time": "2024-11-18T21:32:04.566463Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the model from a file\n",
    "model = Word2Vec.load(\"cnn_w2v.model\")\n",
    "\n",
    "# Now you can use the model\n",
    "print(model.wv.most_similar(\"israeli\"))  # Replace \"your_word\" with the word you're interested in\n",
    "\n",
    "# Load the word vectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format(\"cnn_W2v_vectors.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07a9ff1cc647a56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T22:38:32.738420Z",
     "start_time": "2024-11-18T22:38:32.728437Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the vector for a word\n",
    "vector = model.wv[\"idf\"]\n",
    "\n",
    "# Find most similar words\n",
    "similar_words = model.wv.most_similar(\"bad\")\n",
    "print(similar_words)\n",
    "\n",
    "# Calculate similarity\n",
    "similarity = model.wv.similarity(\"palestine\", \"victim\")\n",
    "print(f\"Similarity between 'palestine' and 'victim': {similarity}\")\n",
    "\n",
    "# Calculate similarity\n",
    "similarity = model.wv.similarity(\"israel\", \"victim\")\n",
    "print(f\"Similarity between 'israel' and 'victim': {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8fba3574ed1af",
   "metadata": {},
   "source": [
    "### Potential Portrayal Words\n",
    "Positive: positive, good, victim, humane, heroic, brave, noble, resilient, justified, courageous, victorious, liberating, righteous, defenders\n",
    "Negative: negative, bad, aggressor, attacker, aggressive, brutal, oppressive, merciless, barbaric, ruthless, massacra\n",
    "invaders, terrorist\n",
    "terroristic, dictatorial, destructive, illegal, corrupt, authoritarian, regressive, settler\n",
    "\n",
    "Find word frequency for these words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d01fda58d2b0e2",
   "metadata": {},
   "source": [
    "\n",
    "# MASTER FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce08ef33bc2ecea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T22:23:01.244548Z",
     "start_time": "2024-11-24T22:23:00.696592Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"data/news-data-extracted.json\", \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "newspaper = data[\"cnn.com\"] # newspaper will be a dictionary of articles with values being url, date, authors, text etc.\n",
    "newspaper_articles = []\n",
    "\n",
    "for article in newspaper:\n",
    "    newspaper_articles.append(article[\"text\"])\n",
    "\n",
    "print(newspaper_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4d516d852bbf82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T22:21:58.624609Z",
     "start_time": "2024-11-24T22:21:58.478317Z"
    }
   },
   "outputs": [],
   "source": [
    "newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5006b0536f5c4d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a scraped newspaper in, which will be a key to the dictionary we download: example: data['cnn.com']\n",
    "# get corpus size before preprocessing\n",
    "# preprocess the newspaper\n",
    "# get corpus size after preprocessing\n",
    "# find word frequency for israel, palestine, idf, hamas, gaza, west bank\n",
    "# save it to the preprocessed_newspaper_articles dictionary\n",
    "# train word2vec on it\n",
    "# measure portrayal for both sides\n",
    "# all this metadata & results in a dict, and preprocessed corpus to preprocessed_newspaper_articles\n",
    "def master(extracted_file, preprocessed_file, newspaper_name):\n",
    "    \n",
    "    import json\n",
    "    with open(extracted_file, \"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "        \n",
    "    newspaper = data[f\"{newspaper_name}\"] # newspaper will be a list of dictionaries, each dictionary representing an article with keys being url, date, authors, text etc.\n",
    "    newspaper_articles = newspaper['text'] # newspaper_articles will be \n",
    "    \n",
    "    \n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
