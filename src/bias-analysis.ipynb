{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eaba59c3bf0d827",
   "metadata": {},
   "source": [
    "What we need to do: \n",
    "1) Go through each newspaper and create a newspaper corpora, display the length of each corpora\n",
    "2) Find either a) wikipedia dataset and train word2vec on it to compare non-context related words like \"bad\", \n",
    "\"victim\", \"good\", \"attacker\" to representation of each side of the conflict in the newspaper corpora, b) find a \n",
    "pretrained word2vec to do the same\n",
    "3) Find a way to classify each article as pro-israel or pro-palestine"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Access and preprocess our data",
   "id": "8de74c7d7aa56c9d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open('data/news-data-extracted.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Print the data\n",
    "first_article_data = data[\"cnn.com\"][0] #cnn is the key to a value which is a list of dictionaries, we get the first dictionary (article) of that list of dictionary\n",
    "first_article = first_article_data[\"text\"]\n",
    "print(first_article_data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d65473b172ae9cd",
   "metadata": {},
   "source": [
    "first_article[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "309ac0a633e6010e",
   "metadata": {},
   "source": [
    "from gensim.utils import tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# tokenizes article into sentences, which are also tokenized into words\n",
    "def tokenize_article(article):\n",
    "    tokenized_article = []\n",
    "    sentences  = sent_tokenize(article, language=\"english\") # divide article into sentences\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokenized_sentence = tokenize(sentence) # divide sentences into words\n",
    "        tokenized_article.append(tokenized_sentence) \n",
    "    return tokenized_article\n",
    "\n",
    "# makes each word lowercase\n",
    "def lowercase(tokenized_article):\n",
    "    lowercase_article = []\n",
    "\n",
    "    for sentence in tokenized_article:\n",
    "        current_sentence = []\n",
    "        for word in sentence:\n",
    "            current_sentence.append(word.lower())\n",
    "        lowercase_article.append(current_sentence)\n",
    "\n",
    "    return lowercase_article\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(tokenized_article):\n",
    "    # Iterate over the index and content of each sentence\n",
    "    for i in range(len(tokenized_article)):\n",
    "        # Create a new list for the filtered sentence\n",
    "        filtered_sentence = []\n",
    "        for word in tokenized_article[i]:\n",
    "            if word not in stop_words:\n",
    "                filtered_sentence.append(word)\n",
    "        # Replace the original sentence with the filtered sentence\n",
    "        tokenized_article[i] = filtered_sentence\n",
    "    return tokenized_article\n",
    "\n",
    "def lammetization(tokenized_article):\n",
    "    lammetizer = WordNetLemmatizer()\n",
    "\n",
    "    lammetized_article = []\n",
    "\n",
    "    for sentence in tokenized_article:\n",
    "        current_sentence = []\n",
    "        for word in sentence:\n",
    "            current_sentence.append(lammetizer.lemmatize(word))\n",
    "        lammetized_article.append(current_sentence)\n",
    "\n",
    "    return lammetized_article\n",
    "\n",
    "\n",
    "def remove_punctuation(tokenized_article):\n",
    "    punc_removed_article = []\n",
    "\n",
    "    for sentence in tokenized_article:\n",
    "        punc_removed_sentence = []\n",
    "        for word in sentence:\n",
    "            # Split by punctuation, filter out empty strings, and join back if needed\n",
    "            split_word = ''.join(re.split(r\"[^\\w]+\", word))\n",
    "            if split_word:  # Add non-empty words only\n",
    "                punc_removed_sentence.append(split_word)\n",
    "\n",
    "        punc_removed_article.append(punc_removed_sentence)\n",
    "\n",
    "    return punc_removed_article\n",
    "\n",
    "def preprocess_article(article):\n",
    "    t_article = tokenize_article(article)\n",
    "    l_article = lowercase(t_article)\n",
    "    r_article = remove_stopwords(l_article)\n",
    "    la_article = lammetization(r_article)\n",
    "    re_article = remove_punctuation(la_article)\n",
    "    return re_article"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(stop_words)\n",
    "print(stopwords.words(\"english\"))\n",
    "print(set(stopwords.words(\"english\")))"
   ],
   "id": "6278a1090040ea31",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e6a5c2a3d799cc88",
   "metadata": {},
   "source": [
    "preprocessed = preprocess_article(first_article)\n",
    "print(preprocessed)\n",
    "for word in stopwords.words(\"english\"):\n",
    "    if word in preprocessed:\n",
    "        print(word)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "80ae5791093d845d",
   "metadata": {},
   "source": [
    "Now create a function that preperocesses a newspaper"
   ]
  },
  {
   "cell_type": "code",
   "id": "94847e14cc9be247",
   "metadata": {},
   "source": [
    "def preprocess_newspaper(newspaper, newspaper_name, newspaper_dict):\n",
    "    newspaper_dict[f\"{newspaper_name}\"] = []\n",
    "    i = 0\n",
    "    for article_data in newspaper:\n",
    "        text = article_data['text']\n",
    "        newspaper_dict[f\"{newspaper_name}\"].extend(preprocess_article(text))  # extends preproccessed\n",
    "        # articles to\n",
    "        # newspaper's article list\n",
    "        print(f\"{newspaper_name}: article {i} preprocessed\")\n",
    "        i += 1\n",
    "    return newspaper_dict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "53184256486a4c43",
   "metadata": {},
   "source": [
    "# Lets try with CNN\n",
    "newspaper = data[\"cnn.com\"]\n",
    "newspaper_dict = {}\n",
    "newspaper_dict = preprocess_newspaper(newspaper, \"cnn.com\", newspaper_dict)\n",
    "print(newspaper_dict)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Save and load preprocessed newspapers",
   "id": "cea70a66012054ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def save_newspaper_dict(newspaper_dict):\n",
    "    # File path for the JSON file\n",
    "    file_path = \"preprocessed_newspaper_articles.json\"\n",
    "\n",
    "    # Step 1: Load existing data if the file exists, otherwise start with an empty list\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as json_file:\n",
    "            data = json.load(json_file)  # Load existing data\n",
    "        for key,value in newspaper_dict:\n",
    "            if key not in data:\n",
    "                data[\"key\"] = value\n",
    "\n",
    "    else:\n",
    "        data = newspaper_dict\n",
    "\n",
    "    # Step 3: Write the updated data back to the file\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n"
   ],
   "id": "5c82d0b37ad9001e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "save_newspaper_dict(newspaper_dict)",
   "id": "cc150acbd50f871c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "with open(\"preprocessed_newspaper_articles.json\", \"r\") as json_file:\n",
    "    loaded_newspaper_dict = json.load(json_file)\n",
    "    print(loaded_newspaper_dict)"
   ],
   "id": "75cfbdd9c26d6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train word2vec on cnn.com",
   "id": "1308917cf59f87fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Prepare sentences for Word2Vec\n",
    "sentences = loaded_newspaper_dict[\"cnn.com\"] # Each newspaper's corpus is one \"document\"\n",
    "print(sentences)\n",
    "# Train Word2Vec model\n",
    "# Initialize the model with parameters\n",
    "model = Word2Vec(sentences=sentences, vector_size=300, window=5, min_count=10, sg=1, workers=4, negative=20)\n",
    "\n",
    "# Train the model\n",
    "model.train(sentences, total_examples=len(sentences), epochs=20)"
   ],
   "id": "fc38a9adcb59c045",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# model.save(\"cnn_w2v.model\")\n",
    "# Save just the word vectors in a text format\n",
    "model.wv.save_word2vec_format(\"cnn_w2v_vectors.txt\", binary=False)\n",
    "\n",
    "# To save in binary format:\n",
    "model.wv.save_word2vec_format(\"cnn_w2v_vectors.bin\", binary=True)\n"
   ],
   "id": "bca1cb1d25189a35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load the model",
   "id": "cd50ddc78675e441"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the word vectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format(\"word2vec_vectors.txt\", binary=False)"
   ],
   "id": "4f2e49512f1caad2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the vector for a word\n",
    "vector = model.wv[\"idf\"]\n",
    "\n",
    "# Find most similar words\n",
    "similar_words = model.wv.most_similar(\"genocide\")\n",
    "print(similar_words)\n",
    "\n",
    "# Calculate similarity\n",
    "similarity = model.wv.similarity(\"hamas\", \"terrorist\")\n",
    "print(f\"Similarity between 'hamas' and 'terrorist': {similarity}\")\n",
    "\n",
    "# Calculate similarity\n",
    "similarity = model.wv.similarity(\"idf\", \"terrorist\")\n",
    "print(f\"Similarity between 'idf' and 'terrorist': {similarity}\")"
   ],
   "id": "a07a9ff1cc647a56",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
